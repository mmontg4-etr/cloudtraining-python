{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Writing Data\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Read and write CSV files with various options\n",
    "2. Work with Excel files (single and multiple sheets)\n",
    "3. Read and write JSON data\n",
    "4. Understand basics of SQL database connections\n",
    "5. Handle common file reading issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import pandas and create some sample data that we'll save to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO  # For simulating file content\n",
    "import json\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for demonstrations\n",
    "sample_data = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'department': ['Engineering', 'Marketing', 'Engineering', 'HR', 'Marketing'],\n",
    "    'salary': [75000, 65000, 80000, 60000, 70000],\n",
    "    'hire_date': ['2020-01-15', '2019-06-01', '2021-03-20', '2018-11-10', '2022-02-28']\n",
    "})\n",
    "print(\"Sample Data:\")\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. CSV Files\n",
    "\n",
    "CSV (Comma-Separated Values) is the most common format for tabular data. Pandas provides `read_csv()` and `to_csv()` for reading and writing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Writing to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV\n",
    "sample_data.to_csv('employees.csv', index=False)\n",
    "print(\"File saved: employees.csv\")\n",
    "\n",
    "# Let's see what the file looks like\n",
    "with open('employees.csv', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write with index included\n",
    "sample_data.to_csv('employees_with_index.csv', index=True)\n",
    "print(\"File saved with index:\")\n",
    "with open('employees_with_index.csv', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reading from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic reading\n",
    "df = pd.read_csv('employees.csv')\n",
    "print(\"Read from CSV:\")\n",
    "print(df)\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 CSV Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate CSV with different separators and options\n",
    "csv_content = \"\"\"name;age;city;score\n",
    "Alice;25;New York;85.5\n",
    "Bob;30;Los Angeles;92.0\n",
    "Charlie;35;Chicago;78.5\n",
    "\"\"\"\n",
    "\n",
    "# Read with semicolon separator\n",
    "df = pd.read_csv(StringIO(csv_content), sep=';')\n",
    "print(\"CSV with semicolon separator:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read specific columns\n",
    "df = pd.read_csv('employees.csv', usecols=['name', 'salary'])\n",
    "print(\"Only name and salary columns:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read with custom column names\n",
    "csv_no_header = \"\"\"1,Alice,Engineering,75000\n",
    "2,Bob,Marketing,65000\n",
    "3,Charlie,Engineering,80000\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv(StringIO(csv_no_header), \n",
    "                 names=['id', 'name', 'dept', 'salary'],\n",
    "                 header=None)\n",
    "print(\"CSV without header (custom column names):\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates automatically\n",
    "df = pd.read_csv('employees.csv', parse_dates=['hire_date'])\n",
    "print(\"With parsed dates:\")\n",
    "print(df)\n",
    "print(f\"\\nhire_date type: {df['hire_date'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a column as index\n",
    "df = pd.read_csv('employees.csv', index_col='id')\n",
    "print(\"With id as index:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read only first N rows (useful for large files)\n",
    "df = pd.read_csv('employees.csv', nrows=3)\n",
    "print(\"First 3 rows only:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV with missing values\n",
    "csv_with_missing = \"\"\"name,age,city,score\n",
    "Alice,25,New York,85.5\n",
    "Bob,,Los Angeles,92.0\n",
    "Charlie,35,,78.5\n",
    "Diana,28,Houston,\n",
    "Eve,NA,Phoenix,88.0\n",
    "\"\"\"\n",
    "\n",
    "# Read with custom NA values\n",
    "df = pd.read_csv(StringIO(csv_with_missing), na_values=['NA', ''])\n",
    "print(\"CSV with missing values:\")\n",
    "print(df)\n",
    "print(f\"\\nNull counts:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Excel Files\n",
    "\n",
    "Pandas can read and write Excel files using `read_excel()` and `to_excel()`. Note: You may need to install `openpyxl` for `.xlsx` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if openpyxl is available\n",
    "try:\n",
    "    import openpyxl\n",
    "    print(f\"openpyxl version: {openpyxl.__version__}\")\n",
    "    EXCEL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"openpyxl not installed. Install with: pip install openpyxl\")\n",
    "    EXCEL_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Writing to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXCEL_AVAILABLE:\n",
    "    # Write to Excel\n",
    "    sample_data.to_excel('employees.xlsx', index=False, sheet_name='Employees')\n",
    "    print(\"File saved: employees.xlsx\")\n",
    "else:\n",
    "    print(\"Skipping Excel write (openpyxl not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXCEL_AVAILABLE:\n",
    "    # Write multiple sheets to one Excel file\n",
    "    with pd.ExcelWriter('multi_sheet.xlsx') as writer:\n",
    "        sample_data.to_excel(writer, sheet_name='Employees', index=False)\n",
    "        \n",
    "        # Create another DataFrame for a second sheet\n",
    "        departments = pd.DataFrame({\n",
    "            'department': ['Engineering', 'Marketing', 'HR'],\n",
    "            'budget': [500000, 300000, 200000],\n",
    "            'headcount': [50, 30, 20]\n",
    "        })\n",
    "        departments.to_excel(writer, sheet_name='Departments', index=False)\n",
    "    \n",
    "    print(\"File saved: multi_sheet.xlsx with 2 sheets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Reading from Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXCEL_AVAILABLE:\n",
    "    # Basic reading\n",
    "    df = pd.read_excel('employees.xlsx')\n",
    "    print(\"Read from Excel:\")\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Skipping Excel read (openpyxl not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXCEL_AVAILABLE:\n",
    "    # Read specific sheet\n",
    "    employees = pd.read_excel('multi_sheet.xlsx', sheet_name='Employees')\n",
    "    departments = pd.read_excel('multi_sheet.xlsx', sheet_name='Departments')\n",
    "    \n",
    "    print(\"Employees sheet:\")\n",
    "    print(employees)\n",
    "    print(\"\\nDepartments sheet:\")\n",
    "    print(departments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXCEL_AVAILABLE:\n",
    "    # Read all sheets into a dictionary\n",
    "    all_sheets = pd.read_excel('multi_sheet.xlsx', sheet_name=None)\n",
    "    print(f\"Sheet names: {list(all_sheets.keys())}\")\n",
    "    print(f\"Type: {type(all_sheets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. JSON Files\n",
    "\n",
    "JSON (JavaScript Object Notation) is common for web APIs and configuration files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Writing to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to JSON (default orientation: columns)\n",
    "sample_data.to_json('employees.json')\n",
    "print(\"File saved: employees.json\")\n",
    "\n",
    "with open('employees.json', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write with different orientations\n",
    "# 'records' orientation - list of dictionaries (common for APIs)\n",
    "sample_data.to_json('employees_records.json', orient='records', indent=2)\n",
    "print(\"Records orientation:\")\n",
    "with open('employees_records.json', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'index' orientation - dictionary keyed by index\n",
    "sample_data.head(2).to_json('employees_index.json', orient='index', indent=2)\n",
    "print(\"Index orientation:\")\n",
    "with open('employees_index.json', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Reading from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON with default orientation\n",
    "df = pd.read_json('employees.json')\n",
    "print(\"Read from JSON (columns orientation):\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON with records orientation\n",
    "df = pd.read_json('employees_records.json', orient='records')\n",
    "print(\"Read from JSON (records orientation):\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Nested JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling nested JSON\n",
    "nested_json = \"\"\"[\n",
    "    {\"name\": \"Alice\", \"info\": {\"age\": 25, \"city\": \"NYC\"}, \"scores\": [85, 90, 88]},\n",
    "    {\"name\": \"Bob\", \"info\": {\"age\": 30, \"city\": \"LA\"}, \"scores\": [92, 88, 95]},\n",
    "    {\"name\": \"Charlie\", \"info\": {\"age\": 35, \"city\": \"Chicago\"}, \"scores\": [78, 82, 80]}\n",
    "]\"\"\"\n",
    "\n",
    "# Basic read keeps nested structure\n",
    "df = pd.read_json(StringIO(nested_json))\n",
    "print(\"Nested JSON (basic read):\")\n",
    "print(df)\n",
    "print(f\"\\ninfo column type: {type(df['info'].iloc[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use json_normalize to flatten nested JSON\n",
    "data = json.loads(nested_json)\n",
    "df_flat = pd.json_normalize(data)\n",
    "print(\"Flattened JSON:\")\n",
    "print(df_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. SQL Databases\n",
    "\n",
    "Pandas can interact with SQL databases using `read_sql()` and `to_sql()`. We'll demonstrate with SQLite, which is built into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create an in-memory SQLite database\n",
    "conn = sqlite3.connect(':memory:')\n",
    "\n",
    "# Write DataFrame to SQL table\n",
    "sample_data.to_sql('employees', conn, index=False, if_exists='replace')\n",
    "print(\"Data written to SQL table 'employees'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read entire table\n",
    "df = pd.read_sql('SELECT * FROM employees', conn)\n",
    "print(\"Read from SQL:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read with SQL query\n",
    "query = \"\"\"\n",
    "SELECT name, department, salary \n",
    "FROM employees \n",
    "WHERE salary > 65000\n",
    "ORDER BY salary DESC\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, conn)\n",
    "print(\"Filtered SQL query result:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation with SQL\n",
    "query = \"\"\"\n",
    "SELECT department, \n",
    "       COUNT(*) as employee_count,\n",
    "       AVG(salary) as avg_salary\n",
    "FROM employees \n",
    "GROUP BY department\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, conn)\n",
    "print(\"SQL aggregation:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Other Formats\n",
    "\n",
    "Pandas supports many other formats. Here are a few quick examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Parquet (Efficient Binary Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if pyarrow is available\n",
    "try:\n",
    "    import pyarrow\n",
    "    \n",
    "    # Write to Parquet\n",
    "    sample_data.to_parquet('employees.parquet')\n",
    "    print(\"Written to Parquet\")\n",
    "    \n",
    "    # Read from Parquet\n",
    "    df = pd.read_parquet('employees.parquet')\n",
    "    print(\"Read from Parquet:\")\n",
    "    print(df)\n",
    "except ImportError:\n",
    "    print(\"pyarrow not installed. Install with: pip install pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Clipboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy DataFrame to clipboard (useful for pasting into Excel)\n",
    "# sample_data.to_clipboard(index=False)\n",
    "\n",
    "# Read from clipboard\n",
    "# df = pd.read_clipboard()\n",
    "\n",
    "print(\"Clipboard operations available:\")\n",
    "print(\"  df.to_clipboard() - copy to clipboard\")\n",
    "print(\"  pd.read_clipboard() - read from clipboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 HTML Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to HTML\n",
    "html = sample_data.to_html(index=False)\n",
    "print(\"HTML output (first 500 chars):\")\n",
    "print(html[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Practice reading and writing data with these exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Create and Save CSV\n",
    "\n",
    "1. Create a DataFrame with the following data about products:\n",
    "   - Product: Laptop, Phone, Tablet, Watch, Headphones\n",
    "   - Price: 999.99, 699.99, 449.99, 299.99, 149.99\n",
    "   - Stock: 50, 150, 80, 200, 300\n",
    "   - Category: Electronics, Electronics, Electronics, Wearables, Audio\n",
    "\n",
    "2. Save it to 'products.csv' without the index\n",
    "3. Read it back and verify the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "# 1. Create DataFrame\n",
    "products = pd.DataFrame({\n",
    "    'Product': ['Laptop', 'Phone', 'Tablet', 'Watch', 'Headphones'],\n",
    "    'Price': [999.99, 699.99, 449.99, 299.99, 149.99],\n",
    "    'Stock': [50, 150, 80, 200, 300],\n",
    "    'Category': ['Electronics', 'Electronics', 'Electronics', 'Wearables', 'Audio']\n",
    "})\n",
    "print(\"Products DataFrame:\")\n",
    "print(products)\n",
    "\n",
    "# 2. Save to CSV\n",
    "products.to_csv('products.csv', index=False)\n",
    "print(\"\\nSaved to products.csv\")\n",
    "\n",
    "# 3. Read back and verify\n",
    "products_loaded = pd.read_csv('products.csv')\n",
    "print(\"\\nLoaded from CSV:\")\n",
    "print(products_loaded)\n",
    "print(f\"\\nData types:\\n{products_loaded.dtypes}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Parse CSV with Options\n",
    "\n",
    "Read the following CSV content (stored as a string) with appropriate options:\n",
    "- Use semicolon as separator\n",
    "- Parse the 'date' column as datetime\n",
    "- Set 'order_id' as the index\n",
    "\n",
    "```python\n",
    "csv_content = \"\"\"order_id;customer;date;amount\n",
    "1001;John;2024-01-15;150.00\n",
    "1002;Jane;2024-01-16;225.50\n",
    "1003;Bob;2024-01-17;89.99\n",
    "1004;Alice;2024-01-18;312.00\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "csv_content = \"\"\"order_id;customer;date;amount\n",
    "1001;John;2024-01-15;150.00\n",
    "1002;Jane;2024-01-16;225.50\n",
    "1003;Bob;2024-01-17;89.99\n",
    "1004;Alice;2024-01-18;312.00\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\n",
    "    StringIO(csv_content),\n",
    "    sep=';',\n",
    "    parse_dates=['date'],\n",
    "    index_col='order_id'\n",
    ")\n",
    "\n",
    "print(\"Parsed DataFrame:\")\n",
    "print(df)\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "print(f\"\\nIndex: {df.index}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: JSON Operations\n",
    "\n",
    "1. Create a DataFrame with 3 students and their grades in Math, Science, and English\n",
    "2. Save it to JSON using 'records' orientation\n",
    "3. Read it back and verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "# 1. Create DataFrame\n",
    "students = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Math': [92, 85, 78],\n",
    "    'Science': [88, 90, 82],\n",
    "    'English': [95, 87, 91]\n",
    "})\n",
    "print(\"Students DataFrame:\")\n",
    "print(students)\n",
    "\n",
    "# 2. Save to JSON with records orientation\n",
    "students.to_json('students.json', orient='records', indent=2)\n",
    "print(\"\\nSaved to students.json\")\n",
    "\n",
    "# Show the file content\n",
    "with open('students.json', 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "# 3. Read back\n",
    "students_loaded = pd.read_json('students.json', orient='records')\n",
    "print(\"\\nLoaded from JSON:\")\n",
    "print(students_loaded)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: SQL Query\n",
    "\n",
    "1. Create an in-memory SQLite database\n",
    "2. Save the products DataFrame (from Exercise 1) to a table called 'products'\n",
    "3. Write a SQL query to find all products with price > 200 and stock > 100\n",
    "4. Read the results into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "# Recreate products DataFrame\n",
    "products = pd.DataFrame({\n",
    "    'Product': ['Laptop', 'Phone', 'Tablet', 'Watch', 'Headphones'],\n",
    "    'Price': [999.99, 699.99, 449.99, 299.99, 149.99],\n",
    "    'Stock': [50, 150, 80, 200, 300],\n",
    "    'Category': ['Electronics', 'Electronics', 'Electronics', 'Wearables', 'Audio']\n",
    "})\n",
    "\n",
    "# 1. Create SQLite connection\n",
    "conn = sqlite3.connect(':memory:')\n",
    "\n",
    "# 2. Save to SQL\n",
    "products.to_sql('products', conn, index=False, if_exists='replace')\n",
    "print(\"Data saved to SQL table 'products'\")\n",
    "\n",
    "# 3 & 4. Query and read results\n",
    "query = \"\"\"\n",
    "SELECT * FROM products\n",
    "WHERE Price > 200 AND Stock > 100\n",
    "\"\"\"\n",
    "result = pd.read_sql(query, conn)\n",
    "print(\"\\nProducts with price > 200 and stock > 100:\")\n",
    "print(result)\n",
    "\n",
    "conn.close()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Handle Nested JSON\n",
    "\n",
    "Parse the following nested JSON and flatten it into a DataFrame:\n",
    "\n",
    "```python\n",
    "nested_data = [\n",
    "    {\"id\": 1, \"user\": {\"name\": \"Alice\", \"email\": \"alice@example.com\"}, \"active\": True},\n",
    "    {\"id\": 2, \"user\": {\"name\": \"Bob\", \"email\": \"bob@example.com\"}, \"active\": False},\n",
    "    {\"id\": 3, \"user\": {\"name\": \"Charlie\", \"email\": \"charlie@example.com\"}, \"active\": True}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "nested_data = [\n",
    "    {\"id\": 1, \"user\": {\"name\": \"Alice\", \"email\": \"alice@example.com\"}, \"active\": True},\n",
    "    {\"id\": 2, \"user\": {\"name\": \"Bob\", \"email\": \"bob@example.com\"}, \"active\": False},\n",
    "    {\"id\": 3, \"user\": {\"name\": \"Charlie\", \"email\": \"charlie@example.com\"}, \"active\": True}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "# Use json_normalize to flatten\n",
    "df_flat = pd.json_normalize(nested_data)\n",
    "print(\"Flattened DataFrame:\")\n",
    "print(df_flat)\n",
    "print(f\"\\nColumns: {df_flat.columns.tolist()}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up created files\n",
    "import os\n",
    "\n",
    "files_to_remove = [\n",
    "    'employees.csv', 'employees_with_index.csv',\n",
    "    'employees.xlsx', 'multi_sheet.xlsx',\n",
    "    'employees.json', 'employees_records.json', 'employees_index.json',\n",
    "    'products.csv', 'students.json', 'employees.parquet'\n",
    "]\n",
    "\n",
    "for f in files_to_remove:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "        print(f\"Removed: {f}\")\n",
    "\n",
    "print(\"\\nCleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **CSV Files**:\n",
    "   - `read_csv()` with options: sep, usecols, parse_dates, index_col, nrows, na_values\n",
    "   - `to_csv()` with index option\n",
    "\n",
    "2. **Excel Files**:\n",
    "   - `read_excel()` for single/multiple sheets\n",
    "   - `to_excel()` with ExcelWriter for multiple sheets\n",
    "   - Requires `openpyxl` package\n",
    "\n",
    "3. **JSON Files**:\n",
    "   - `read_json()` and `to_json()` with different orientations\n",
    "   - `json_normalize()` for nested JSON\n",
    "\n",
    "4. **SQL Databases**:\n",
    "   - `read_sql()` for queries\n",
    "   - `to_sql()` for writing tables\n",
    "   - Works with any database via SQLAlchemy\n",
    "\n",
    "5. **Other Formats**: Parquet, clipboard, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to the next notebook: **[03_indexing_and_selection.ipynb](03_indexing_and_selection.ipynb)** to learn how to select and filter data using loc, iloc, and boolean indexing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
