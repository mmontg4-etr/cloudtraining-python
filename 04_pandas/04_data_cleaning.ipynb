{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Detect and handle missing data (NaN values)\n",
    "2. Identify and remove duplicate rows\n",
    "3. Convert and manage data types\n",
    "4. Use string methods for text data cleaning\n",
    "5. Apply transformations to clean and standardize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messy DataFrame for cleaning exercises\n",
    "messy_data = pd.DataFrame({\n",
    "    'name': ['  Alice  ', 'BOB', 'charlie', 'Diana', 'eve', 'Alice', None, 'Frank'],\n",
    "    'email': ['alice@email.com', 'bob@EMAIL.COM', 'charlie@email.com', None, 'eve@email.com', 'alice@email.com', 'grace@email.com', 'FRANK@email.com'],\n",
    "    'age': ['25', '30', 'thirty-five', '28', '32', '25', '40', None],\n",
    "    'salary': [50000, 60000, None, 55000, np.nan, 50000, 70000, 65000],\n",
    "    'department': ['Sales', 'MARKETING', 'sales', 'Engineering', 'Marketing', 'Sales', 'HR', 'engineering'],\n",
    "    'hire_date': ['2020-01-15', '2019/06/01', '2021-03-20', '20-11-2018', '2022-02-28', '2020-01-15', '2017-08-10', 'invalid']\n",
    "})\n",
    "\n",
    "print(\"Messy Data:\")\n",
    "print(messy_data)\n",
    "print(f\"\\nData types:\\n{messy_data.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Handling Missing Data\n",
    "\n",
    "Missing data in Pandas is represented as `NaN` (Not a Number) or `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Detecting Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(messy_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percentage of missing values\n",
    "missing_pct = (messy_data.isnull().sum() / len(messy_data)) * 100\n",
    "print(\"Missing value percentage:\")\n",
    "print(missing_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any value is missing in each row\n",
    "print(\"Rows with any missing values:\")\n",
    "print(messy_data[messy_data.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isna() is an alias for isnull()\n",
    "print(\"Total missing values:\", messy_data.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dropping Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with ANY missing values\n",
    "cleaned = messy_data.dropna()\n",
    "print(f\"Original rows: {len(messy_data)}, After dropna: {len(cleaned)}\")\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where ALL values are missing\n",
    "cleaned = messy_data.dropna(how='all')\n",
    "print(f\"Rows after dropping all-NA rows: {len(cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in specific columns\n",
    "cleaned = messy_data.dropna(subset=['name', 'email'])\n",
    "print(\"After dropping rows missing name or email:\")\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with missing values\n",
    "cleaned = messy_data.dropna(axis=1)\n",
    "print(\"Columns without missing values:\")\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with at least N non-null values (thresh)\n",
    "cleaned = messy_data.dropna(thresh=5)  # Keep rows with at least 5 non-null values\n",
    "print(f\"Rows with at least 5 non-null values: {len(cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simpler DataFrame for fill examples\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 2, 3, np.nan, 5],\n",
    "    'C': ['x', None, 'y', 'z', None]\n",
    "})\n",
    "print(\"Original:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with a constant value\n",
    "print(\"Fill with 0:\")\n",
    "print(df.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with different values per column\n",
    "fill_values = {'A': 0, 'B': -1, 'C': 'unknown'}\n",
    "print(\"Fill with different values:\")\n",
    "print(df.fillna(fill_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with statistics (mean, median, mode)\n",
    "df_numeric = df[['A', 'B']].copy()\n",
    "print(\"Fill with mean:\")\n",
    "print(df_numeric.fillna(df_numeric.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill (use previous value)\n",
    "print(\"Forward fill (ffill):\")\n",
    "print(df.ffill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward fill (use next value)\n",
    "print(\"Backward fill (bfill):\")\n",
    "print(df.bfill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate numeric values\n",
    "print(\"Interpolate:\")\n",
    "print(df[['A', 'B']].interpolate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Handling Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the messy data for duplicates\n",
    "print(\"Original data:\")\n",
    "print(messy_data[['name', 'email', 'age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Detecting Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print(\"Duplicate rows:\")\n",
    "print(messy_data.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show duplicate rows\n",
    "print(\"Rows that are duplicates:\")\n",
    "print(messy_data[messy_data.duplicated(keep=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicates in specific columns\n",
    "print(\"Duplicate emails:\")\n",
    "print(messy_data[messy_data.duplicated(subset=['email'], keep=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows (keep first occurrence)\n",
    "cleaned = messy_data.drop_duplicates()\n",
    "print(f\"Original: {len(messy_data)}, After removing duplicates: {len(cleaned)}\")\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates based on specific columns\n",
    "cleaned = messy_data.drop_duplicates(subset=['email'], keep='first')\n",
    "print(\"After removing duplicate emails (keep first):\")\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep last occurrence instead of first\n",
    "cleaned = messy_data.drop_duplicates(subset=['email'], keep='last')\n",
    "print(\"After removing duplicate emails (keep last):\")\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Type Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current data types\n",
    "print(\"Current data types:\")\n",
    "print(messy_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Converting to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'age' column has mixed types - let's try to convert\n",
    "print(\"Age column values:\")\n",
    "print(messy_data['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_numeric with errors='coerce' converts invalid values to NaN\n",
    "numeric_age = pd.to_numeric(messy_data['age'], errors='coerce')\n",
    "print(\"Converted to numeric (errors='coerce'):\")\n",
    "print(numeric_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column using astype (when data is clean)\n",
    "df = pd.DataFrame({'numbers': ['1', '2', '3', '4', '5']})\n",
    "df['numbers'] = df['numbers'].astype(int)\n",
    "print(f\"Converted type: {df['numbers'].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Converting to DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hire_date column has various formats\n",
    "print(\"Hire date values:\")\n",
    "print(messy_data['hire_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_datetime with errors='coerce'\n",
    "dates = pd.to_datetime(messy_data['hire_date'], errors='coerce')\n",
    "print(\"Converted dates:\")\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify format for consistent dates\n",
    "date_strings = pd.Series(['2020-01-15', '2019-06-01', '2021-03-20'])\n",
    "dates = pd.to_datetime(date_strings, format='%Y-%m-%d')\n",
    "print(\"Dates with format:\")\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date components\n",
    "df = pd.DataFrame({'date': dates})\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_name'] = df['date'].dt.day_name()\n",
    "print(\"Date components:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Converting to Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert department to category type (memory efficient for repeated values)\n",
    "df = messy_data.copy()\n",
    "print(f\"Before: {df['department'].dtype}\")\n",
    "print(f\"Memory: {df['department'].memory_usage(deep=True)} bytes\")\n",
    "\n",
    "df['department'] = df['department'].astype('category')\n",
    "print(f\"\\nAfter: {df['department'].dtype}\")\n",
    "print(f\"Memory: {df['department'].memory_usage(deep=True)} bytes\")\n",
    "print(f\"Categories: {df['department'].cat.categories.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. String Methods\n",
    "\n",
    "Pandas provides the `.str` accessor for string operations on Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean copy for string operations\n",
    "df = messy_data.copy()\n",
    "print(\"Names before cleaning:\")\n",
    "print(df['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "print(\"Lowercase:\")\n",
    "print(df['name'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to uppercase\n",
    "print(\"Uppercase:\")\n",
    "print(df['name'].str.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title case\n",
    "print(\"Title case:\")\n",
    "print(df['name'].str.title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Stripping Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip leading and trailing whitespace\n",
    "print(\"Before strip:\")\n",
    "print(repr(df['name'].iloc[0]))  # repr shows the spaces\n",
    "\n",
    "print(\"\\nAfter strip:\")\n",
    "print(repr(df['name'].str.strip().iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean names: strip whitespace and title case\n",
    "df['name_clean'] = df['name'].str.strip().str.title()\n",
    "print(\"Cleaned names:\")\n",
    "print(df[['name', 'name_clean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 String Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace specific strings\n",
    "df['department_clean'] = df['department'].str.replace('MARKETING', 'Marketing')\n",
    "print(df[['department', 'department_clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better approach: standardize all department names\n",
    "df['department_clean'] = df['department'].str.strip().str.title()\n",
    "print(\"Standardized departments:\")\n",
    "print(df['department_clean'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 String Splitting and Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split email to get username and domain\n",
    "email_parts = df['email'].str.split('@', expand=True)\n",
    "email_parts.columns = ['username', 'domain']\n",
    "print(\"Email parts:\")\n",
    "print(email_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract using regex\n",
    "# Extract just the username part\n",
    "df['username'] = df['email'].str.extract(r'([^@]+)@')\n",
    "print(\"Extracted usernames:\")\n",
    "print(df[['email', 'username']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 String Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if string contains pattern\n",
    "print(\"Contains 'email.com':\")\n",
    "print(df['email'].str.contains('email.com', case=False, na=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if string starts/ends with pattern\n",
    "print(\"Starts with 'a' (case insensitive):\")\n",
    "print(df['name'].str.lower().str.startswith('a', na=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Replacing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace specific values\n",
    "df = pd.DataFrame({\n",
    "    'status': ['active', 'inactive', 'pending', 'Active', 'INACTIVE', 'unknown'],\n",
    "    'grade': ['A', 'B', 'C', 'D', 'F', 'incomplete']\n",
    "})\n",
    "print(\"Original:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace single value\n",
    "df['status_clean'] = df['status'].replace('unknown', np.nan)\n",
    "print(\"After replacing 'unknown':\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace multiple values with a mapping dictionary\n",
    "status_mapping = {\n",
    "    'active': 'Active',\n",
    "    'inactive': 'Inactive',\n",
    "    'Active': 'Active',\n",
    "    'INACTIVE': 'Inactive',\n",
    "    'pending': 'Pending',\n",
    "    'unknown': np.nan\n",
    "}\n",
    "df['status_clean'] = df['status'].replace(status_mapping)\n",
    "print(\"After mapping:\")\n",
    "print(df[['status', 'status_clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace using regex\n",
    "df = pd.DataFrame({'text': ['Price: $100', 'Cost: $250', 'Value: $75']})\n",
    "df['numbers'] = df['text'].str.replace(r'[^\\d]', '', regex=True)\n",
    "print(\"Extract numbers:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Applying Functions for Custom Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a function to a column\n",
    "def clean_name(name):\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    return name.strip().title()\n",
    "\n",
    "df = messy_data.copy()\n",
    "df['name_clean'] = df['name'].apply(clean_name)\n",
    "print(\"Applied custom function:\")\n",
    "print(df[['name', 'name_clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lambda function\n",
    "df['email_lower'] = df['email'].apply(lambda x: x.lower() if pd.notna(x) else x)\n",
    "print(\"Email lowercased:\")\n",
    "print(df[['email', 'email_lower']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to entire DataFrame\n",
    "def clean_string_columns(df):\n",
    "    df_clean = df.copy()\n",
    "    for col in df_clean.select_dtypes(include=['object']).columns:\n",
    "        df_clean[col] = df_clean[col].str.strip().str.lower()\n",
    "    return df_clean\n",
    "\n",
    "# Note: This would need handling for None values in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete data cleaning pipeline\n",
    "def clean_employee_data(df):\n",
    "    \"\"\"\n",
    "    Clean the messy employee data.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Clean names: strip whitespace, title case\n",
    "    df_clean['name'] = df_clean['name'].str.strip().str.title()\n",
    "    \n",
    "    # 2. Clean emails: lowercase\n",
    "    df_clean['email'] = df_clean['email'].str.lower()\n",
    "    \n",
    "    # 3. Convert age to numeric, coercing errors to NaN\n",
    "    df_clean['age'] = pd.to_numeric(df_clean['age'], errors='coerce')\n",
    "    \n",
    "    # 4. Standardize department names\n",
    "    df_clean['department'] = df_clean['department'].str.strip().str.title()\n",
    "    \n",
    "    # 5. Convert hire_date to datetime\n",
    "    df_clean['hire_date'] = pd.to_datetime(df_clean['hire_date'], errors='coerce')\n",
    "    \n",
    "    # 6. Remove duplicate rows based on email\n",
    "    df_clean = df_clean.drop_duplicates(subset=['email'], keep='first')\n",
    "    \n",
    "    # 7. Drop rows missing critical information\n",
    "    df_clean = df_clean.dropna(subset=['name', 'email'])\n",
    "    \n",
    "    # 8. Fill remaining missing values\n",
    "    df_clean['salary'] = df_clean['salary'].fillna(df_clean['salary'].median())\n",
    "    df_clean['age'] = df_clean['age'].fillna(df_clean['age'].median())\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply the cleaning pipeline\n",
    "cleaned_data = clean_employee_data(messy_data)\n",
    "print(\"Cleaned Data:\")\n",
    "print(cleaned_data)\n",
    "print(f\"\\nData types:\\n{cleaned_data.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create exercise data\n",
    "exercise_data = pd.DataFrame({\n",
    "    'product_name': ['  Widget A  ', 'WIDGET B', 'gadget c', 'Widget A', None, 'Gadget D'],\n",
    "    'price': ['19.99', '29.99', 'thirty', '19.99', '49.99', np.nan],\n",
    "    'quantity': [100, None, 50, 100, 75, 25],\n",
    "    'category': ['electronics', 'ELECTRONICS', 'home', 'Electronics', 'Home', 'home'],\n",
    "    'date_added': ['2023-01-15', '2023/02/20', '15-03-2023', '2023-01-15', 'invalid', '2023-04-10']\n",
    "})\n",
    "print(\"Exercise Data:\")\n",
    "print(exercise_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Missing Data Analysis\n",
    "\n",
    "1. Find the total number of missing values in the exercise_data\n",
    "2. Calculate the percentage of missing values for each column\n",
    "3. Identify which rows have any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "# 1. Total missing values\n",
    "total_missing = exercise_data.isnull().sum().sum()\n",
    "print(f\"Total missing values: {total_missing}\")\n",
    "\n",
    "# 2. Percentage missing per column\n",
    "missing_pct = (exercise_data.isnull().sum() / len(exercise_data)) * 100\n",
    "print(f\"\\nMissing percentage per column:\")\n",
    "print(missing_pct)\n",
    "\n",
    "# 3. Rows with any missing values\n",
    "print(f\"\\nRows with missing values:\")\n",
    "print(exercise_data[exercise_data.isnull().any(axis=1)])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Clean Product Names\n",
    "\n",
    "Clean the 'product_name' column:\n",
    "1. Strip whitespace\n",
    "2. Convert to title case\n",
    "3. Handle None values appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "df = exercise_data.copy()\n",
    "df['product_name_clean'] = df['product_name'].str.strip().str.title()\n",
    "print(\"Cleaned product names:\")\n",
    "print(df[['product_name', 'product_name_clean']])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Data Type Conversion\n",
    "\n",
    "1. Convert the 'price' column to numeric (handling errors)\n",
    "2. Convert the 'date_added' column to datetime (handling errors)\n",
    "3. Convert 'category' to a categorical type after standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "df = exercise_data.copy()\n",
    "\n",
    "# 1. Convert price to numeric\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "print(f\"Price dtype: {df['price'].dtype}\")\n",
    "print(df['price'])\n",
    "\n",
    "# 2. Convert date_added to datetime\n",
    "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
    "print(f\"\\nDate dtype: {df['date_added'].dtype}\")\n",
    "print(df['date_added'])\n",
    "\n",
    "# 3. Standardize and convert category\n",
    "df['category'] = df['category'].str.strip().str.title().astype('category')\n",
    "print(f\"\\nCategory dtype: {df['category'].dtype}\")\n",
    "print(f\"Categories: {df['category'].cat.categories.tolist()}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Remove Duplicates\n",
    "\n",
    "1. After cleaning the product names, identify duplicate products\n",
    "2. Remove duplicates, keeping the first occurrence\n",
    "3. Report how many rows were removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "df = exercise_data.copy()\n",
    "\n",
    "# Clean product names first\n",
    "df['product_name'] = df['product_name'].str.strip().str.title()\n",
    "\n",
    "# 1. Identify duplicates\n",
    "print(\"Duplicate products:\")\n",
    "print(df[df.duplicated(subset=['product_name'], keep=False)])\n",
    "\n",
    "# 2. Remove duplicates\n",
    "original_count = len(df)\n",
    "df_deduped = df.drop_duplicates(subset=['product_name'], keep='first')\n",
    "\n",
    "# 3. Report\n",
    "removed = original_count - len(df_deduped)\n",
    "print(f\"\\nOriginal rows: {original_count}\")\n",
    "print(f\"After deduplication: {len(df_deduped)}\")\n",
    "print(f\"Rows removed: {removed}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Complete Cleaning Pipeline\n",
    "\n",
    "Write a function that performs a complete cleaning of the exercise_data:\n",
    "1. Clean product names (strip, title case)\n",
    "2. Convert price to numeric\n",
    "3. Fill missing quantities with 0\n",
    "4. Standardize category names\n",
    "5. Convert dates to datetime\n",
    "6. Remove duplicates based on product name\n",
    "7. Drop rows where product_name is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "def clean_product_data(df):\n",
    "    \"\"\"\n",
    "    Complete cleaning pipeline for product data.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Clean product names\n",
    "    df_clean['product_name'] = df_clean['product_name'].str.strip().str.title()\n",
    "    \n",
    "    # 2. Convert price to numeric\n",
    "    df_clean['price'] = pd.to_numeric(df_clean['price'], errors='coerce')\n",
    "    \n",
    "    # 3. Fill missing quantities with 0\n",
    "    df_clean['quantity'] = df_clean['quantity'].fillna(0).astype(int)\n",
    "    \n",
    "    # 4. Standardize category names\n",
    "    df_clean['category'] = df_clean['category'].str.strip().str.title()\n",
    "    \n",
    "    # 5. Convert dates to datetime\n",
    "    df_clean['date_added'] = pd.to_datetime(df_clean['date_added'], errors='coerce')\n",
    "    \n",
    "    # 6. Remove duplicates\n",
    "    df_clean = df_clean.drop_duplicates(subset=['product_name'], keep='first')\n",
    "    \n",
    "    # 7. Drop rows with null product_name\n",
    "    df_clean = df_clean.dropna(subset=['product_name'])\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply the cleaning function\n",
    "cleaned = clean_product_data(exercise_data)\n",
    "print(\"Cleaned Data:\")\n",
    "print(cleaned)\n",
    "print(f\"\\nData types:\\n{cleaned.dtypes}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Missing Data**:\n",
    "   - Detection: `isnull()`, `isna()`, `sum()`\n",
    "   - Removal: `dropna()` with how, subset, thresh options\n",
    "   - Filling: `fillna()`, `ffill()`, `bfill()`, `interpolate()`\n",
    "\n",
    "2. **Duplicates**:\n",
    "   - Detection: `duplicated()`\n",
    "   - Removal: `drop_duplicates()` with subset, keep options\n",
    "\n",
    "3. **Data Type Conversions**:\n",
    "   - `pd.to_numeric()`, `pd.to_datetime()` with errors='coerce'\n",
    "   - `astype()` for clean data\n",
    "   - Category type for memory efficiency\n",
    "\n",
    "4. **String Methods** (`.str` accessor):\n",
    "   - Case: `lower()`, `upper()`, `title()`\n",
    "   - Whitespace: `strip()`, `lstrip()`, `rstrip()`\n",
    "   - Manipulation: `replace()`, `split()`, `extract()`\n",
    "   - Matching: `contains()`, `startswith()`, `endswith()`\n",
    "\n",
    "5. **Value Replacement**:\n",
    "   - `replace()` with values or dictionaries\n",
    "   - Regex support for pattern matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to the next notebook: **[05_groupby_aggregation.ipynb](05_groupby_aggregation.ipynb)** to learn how to group data and perform aggregations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
