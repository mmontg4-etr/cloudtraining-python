{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Patterns for AWS with Boto3\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Handle AWS errors with botocore exceptions\n",
    "2. Implement pagination for large result sets\n",
    "3. Use async operations for better performance\n",
    "4. Apply best practices for production AWS code\n",
    "5. Implement common design patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Error Handling with Botocore Exceptions\n",
    "\n",
    "### Exception Hierarchy\n",
    "\n",
    "```\n",
    "BaseException\n",
    "  +-- Exception\n",
    "        +-- BotoCoreError (base for all botocore errors)\n",
    "        |     +-- DataNotFoundError\n",
    "        |     +-- UnknownServiceError\n",
    "        |     +-- ApiVersionNotFoundError\n",
    "        |     +-- ...\n",
    "        +-- ClientError (AWS service errors)\n",
    "              +-- Contains Error Code and Message\n",
    "```\n",
    "\n",
    "### Common Error Codes by Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common AWS error codes reference\n",
    "AWS_ERROR_CODES = {\n",
    "    \"S3\": {\n",
    "        \"NoSuchBucket\": \"The specified bucket does not exist\",\n",
    "        \"NoSuchKey\": \"The specified key does not exist\",\n",
    "        \"BucketAlreadyExists\": \"Bucket name already taken globally\",\n",
    "        \"BucketNotEmpty\": \"Cannot delete non-empty bucket\",\n",
    "        \"AccessDenied\": \"Access denied to this resource\",\n",
    "    },\n",
    "    \"DynamoDB\": {\n",
    "        \"ResourceNotFoundException\": \"Table or index doesn't exist\",\n",
    "        \"ConditionalCheckFailedException\": \"Condition expression failed\",\n",
    "        \"ProvisionedThroughputExceededException\": \"Request rate too high\",\n",
    "        \"ValidationException\": \"Invalid parameters\",\n",
    "        \"ResourceInUseException\": \"Table is being created/deleted\",\n",
    "    },\n",
    "    \"Lambda\": {\n",
    "        \"ResourceNotFoundException\": \"Function not found\",\n",
    "        \"InvalidParameterValueException\": \"Invalid parameter\",\n",
    "        \"TooManyRequestsException\": \"Rate limit exceeded\",\n",
    "        \"ServiceException\": \"Lambda service error\",\n",
    "    },\n",
    "    \"SQS\": {\n",
    "        \"QueueDoesNotExist\": \"Queue not found\",\n",
    "        \"QueueDeletedRecently\": \"Queue was recently deleted\",\n",
    "        \"MessageNotInflight\": \"Message not currently in flight\",\n",
    "    },\n",
    "    \"General\": {\n",
    "        \"ExpiredTokenException\": \"Security token has expired\",\n",
    "        \"UnauthorizedAccess\": \"Credentials are invalid\",\n",
    "        \"ThrottlingException\": \"Request rate exceeded\",\n",
    "        \"RequestLimitExceeded\": \"API request limit reached\",\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Common AWS Error Codes:\")\n",
    "print(\"=\" * 60)\n",
    "for service, errors in AWS_ERROR_CODES.items():\n",
    "    print(f\"\\n{service}:\")\n",
    "    for code, description in errors.items():\n",
    "        print(f\"  {code:40} - {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import (\n",
    "    ClientError,\n",
    "    BotoCoreError,\n",
    "    NoCredentialsError,\n",
    "    PartialCredentialsError,\n",
    "    ParamValidationError,\n",
    "    EndpointConnectionError,\n",
    "    ReadTimeoutError,\n",
    "    ConnectTimeoutError\n",
    ")\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "class AWSErrorHandler:\n",
    "    \"\"\"Centralized AWS error handling.\"\"\"\n",
    "    \n",
    "    # Errors that should be retried\n",
    "    RETRYABLE_ERRORS = {\n",
    "        'ProvisionedThroughputExceededException',\n",
    "        'ThrottlingException',\n",
    "        'TooManyRequestsException',\n",
    "        'RequestLimitExceeded',\n",
    "        'ServiceException',\n",
    "        'ServiceUnavailable',\n",
    "        'InternalServerError',\n",
    "        'EC2ThrottledException',\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_retryable(error: ClientError) -> bool:\n",
    "        \"\"\"Check if an error should be retried.\"\"\"\n",
    "        error_code = error.response['Error']['Code']\n",
    "        return error_code in AWSErrorHandler.RETRYABLE_ERRORS\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_error_info(error: ClientError) -> Dict[str, Any]:\n",
    "        \"\"\"Extract error information from ClientError.\"\"\"\n",
    "        return {\n",
    "            'code': error.response['Error']['Code'],\n",
    "            'message': error.response['Error']['Message'],\n",
    "            'request_id': error.response.get('ResponseMetadata', {}).get('RequestId'),\n",
    "            'http_status': error.response.get('ResponseMetadata', {}).get('HTTPStatusCode'),\n",
    "            'retryable': AWSErrorHandler.is_retryable(error)\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_s3_error(error: ClientError, bucket: str = None, key: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Handle S3-specific errors with context.\"\"\"\n",
    "        error_code = error.response['Error']['Code']\n",
    "        \n",
    "        if error_code == 'NoSuchBucket':\n",
    "            return {\n",
    "                'error': 'Bucket not found',\n",
    "                'bucket': bucket,\n",
    "                'suggestion': 'Check bucket name and region'\n",
    "            }\n",
    "        elif error_code == 'NoSuchKey':\n",
    "            return {\n",
    "                'error': 'Object not found',\n",
    "                'bucket': bucket,\n",
    "                'key': key,\n",
    "                'suggestion': 'Verify the object key is correct'\n",
    "            }\n",
    "        elif error_code == 'AccessDenied':\n",
    "            return {\n",
    "                'error': 'Access denied',\n",
    "                'suggestion': 'Check IAM permissions and bucket policy'\n",
    "            }\n",
    "        else:\n",
    "            return AWSErrorHandler.get_error_info(error)\n",
    "\n",
    "print(\"AWSErrorHandler class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive error handling example\n",
    "def safe_s3_get(\n",
    "    bucket: str,\n",
    "    key: str,\n",
    "    s3_client=None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Safely get an object from S3 with comprehensive error handling.\n",
    "    \n",
    "    Args:\n",
    "        bucket: S3 bucket name\n",
    "        key: Object key\n",
    "        s3_client: Optional S3 client (creates one if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with content or error information\n",
    "    \"\"\"\n",
    "    if s3_client is None:\n",
    "        try:\n",
    "            s3_client = boto3.client('s3')\n",
    "        except NoCredentialsError:\n",
    "            return {'success': False, 'error': 'No AWS credentials found'}\n",
    "        except PartialCredentialsError:\n",
    "            return {'success': False, 'error': 'Incomplete AWS credentials'}\n",
    "    \n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "        content = response['Body'].read()\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'content': content,\n",
    "            'content_type': response.get('ContentType'),\n",
    "            'content_length': response.get('ContentLength'),\n",
    "            'last_modified': response.get('LastModified'),\n",
    "            'etag': response.get('ETag')\n",
    "        }\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_info = AWSErrorHandler.handle_s3_error(e, bucket, key)\n",
    "        return {'success': False, **error_info}\n",
    "    \n",
    "    except EndpointConnectionError:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'Cannot connect to AWS endpoint',\n",
    "            'suggestion': 'Check internet connection and region'\n",
    "        }\n",
    "    \n",
    "    except (ReadTimeoutError, ConnectTimeoutError) as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'Request timed out',\n",
    "            'retryable': True\n",
    "        }\n",
    "    \n",
    "    except ParamValidationError as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': f'Invalid parameters: {e}'\n",
    "        }\n",
    "    \n",
    "    except BotoCoreError as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': f'AWS SDK error: {e}'\n",
    "        }\n",
    "\n",
    "print(\"Example: Safe S3 operations\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\"\"\n",
    "result = safe_s3_get('my-bucket', 'data/config.json')\n",
    "\n",
    "if result['success']:\n",
    "    config = json.loads(result['content'])\n",
    "    print(f\"Loaded config: {config}\")\n",
    "else:\n",
    "    print(f\"Error: {result['error']}\")\n",
    "    if result.get('suggestion'):\n",
    "        print(f\"Suggestion: {result['suggestion']}\")\n",
    "    if result.get('retryable'):\n",
    "        print(\"This error can be retried\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry decorator with exponential backoff\n",
    "import time\n",
    "import functools\n",
    "from typing import Callable, Type, Tuple\n",
    "\n",
    "def retry_with_backoff(\n",
    "    max_retries: int = 3,\n",
    "    base_delay: float = 1.0,\n",
    "    max_delay: float = 60.0,\n",
    "    exponential_base: float = 2.0,\n",
    "    retryable_exceptions: Tuple[Type[Exception], ...] = (ClientError,)\n",
    "):\n",
    "    \"\"\"\n",
    "    Decorator for retrying functions with exponential backoff.\n",
    "    \n",
    "    Args:\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        base_delay: Initial delay between retries (seconds)\n",
    "        max_delay: Maximum delay between retries\n",
    "        exponential_base: Base for exponential calculation\n",
    "        retryable_exceptions: Exceptions to retry on\n",
    "    \"\"\"\n",
    "    def decorator(func: Callable):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            last_exception = None\n",
    "            \n",
    "            for attempt in range(max_retries + 1):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                    \n",
    "                except retryable_exceptions as e:\n",
    "                    last_exception = e\n",
    "                    \n",
    "                    # Check if error is retryable\n",
    "                    if isinstance(e, ClientError):\n",
    "                        if not AWSErrorHandler.is_retryable(e):\n",
    "                            raise\n",
    "                    \n",
    "                    if attempt < max_retries:\n",
    "                        # Calculate delay with jitter\n",
    "                        delay = min(\n",
    "                            base_delay * (exponential_base ** attempt),\n",
    "                            max_delay\n",
    "                        )\n",
    "                        # Add jitter (0-25% of delay)\n",
    "                        jitter = delay * 0.25 * (time.time() % 1)\n",
    "                        delay += jitter\n",
    "                        \n",
    "                        print(f\"Attempt {attempt + 1} failed, \"\n",
    "                              f\"retrying in {delay:.2f}s...\")\n",
    "                        time.sleep(delay)\n",
    "                    else:\n",
    "                        print(f\"Max retries ({max_retries}) exceeded\")\n",
    "                        raise\n",
    "            \n",
    "            raise last_exception\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Example usage\n",
    "@retry_with_backoff(max_retries=3, base_delay=1.0)\n",
    "def get_dynamodb_item(table_name: str, key: dict):\n",
    "    \"\"\"Get item from DynamoDB with automatic retries.\"\"\"\n",
    "    dynamodb = boto3.resource('dynamodb')\n",
    "    table = dynamodb.Table(table_name)\n",
    "    response = table.get_item(Key=key)\n",
    "    return response.get('Item')\n",
    "\n",
    "print(\"Retry decorator defined\")\n",
    "print(\"\"\"\n",
    "Usage:\n",
    "@retry_with_backoff(max_retries=3)\n",
    "def my_aws_function():\n",
    "    # AWS operations here\n",
    "    pass\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pagination\n",
    "\n",
    "### Why Pagination?\n",
    "\n",
    "AWS APIs return limited results per request:\n",
    "- S3 `list_objects_v2`: Max 1000 objects\n",
    "- DynamoDB `scan`/`query`: Max 1MB of data\n",
    "- Lambda `list_functions`: Max 50 functions\n",
    "\n",
    "### Pagination Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator, List, Dict, Any\n",
    "\n",
    "# Method 1: Manual pagination with continuation tokens\n",
    "def list_s3_objects_manual(bucket: str, prefix: str = '') -> List[Dict]:\n",
    "    \"\"\"\n",
    "    List S3 objects using manual pagination.\n",
    "    \n",
    "    Shows the low-level approach - good for understanding,\n",
    "    but use paginators in production.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    all_objects = []\n",
    "    continuation_token = None\n",
    "    \n",
    "    while True:\n",
    "        # Build request parameters\n",
    "        params = {\n",
    "            'Bucket': bucket,\n",
    "            'Prefix': prefix,\n",
    "            'MaxKeys': 1000  # Maximum allowed\n",
    "        }\n",
    "        \n",
    "        if continuation_token:\n",
    "            params['ContinuationToken'] = continuation_token\n",
    "        \n",
    "        response = s3.list_objects_v2(**params)\n",
    "        \n",
    "        # Collect objects\n",
    "        for obj in response.get('Contents', []):\n",
    "            all_objects.append({\n",
    "                'key': obj['Key'],\n",
    "                'size': obj['Size'],\n",
    "                'last_modified': obj['LastModified']\n",
    "            })\n",
    "        \n",
    "        # Check for more pages\n",
    "        if response.get('IsTruncated'):\n",
    "            continuation_token = response['NextContinuationToken']\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return all_objects\n",
    "\n",
    "print(\"Manual pagination example defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using paginators (recommended)\n",
    "def list_s3_objects_paginator(bucket: str, prefix: str = '') -> Generator:\n",
    "    \"\"\"\n",
    "    List S3 objects using boto3 paginators.\n",
    "    \n",
    "    This is the recommended approach - cleaner and handles\n",
    "    all pagination logic automatically.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    \n",
    "    # Create page iterator\n",
    "    page_iterator = paginator.paginate(\n",
    "        Bucket=bucket,\n",
    "        Prefix=prefix,\n",
    "        PaginationConfig={\n",
    "            'MaxItems': 10000,  # Total max items\n",
    "            'PageSize': 1000    # Items per page\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Yield objects one at a time\n",
    "    for page in page_iterator:\n",
    "        for obj in page.get('Contents', []):\n",
    "            yield {\n",
    "                'key': obj['Key'],\n",
    "                'size': obj['Size'],\n",
    "                'last_modified': obj['LastModified']\n",
    "            }\n",
    "\n",
    "print(\"Paginator example defined\")\n",
    "print(\"\"\"\n",
    "Usage:\n",
    "for obj in list_s3_objects_paginator('my-bucket', 'data/'):\n",
    "    print(f\"Processing: {obj['key']}\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Using JMESPath for server-side filtering\n",
    "def list_large_objects(\n",
    "    bucket: str,\n",
    "    prefix: str = '',\n",
    "    min_size_mb: int = 100\n",
    ") -> Generator:\n",
    "    \"\"\"\n",
    "    List objects larger than specified size using JMESPath filtering.\n",
    "    \n",
    "    JMESPath filtering happens server-side, reducing data transfer.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    \n",
    "    min_size_bytes = min_size_mb * 1024 * 1024\n",
    "    \n",
    "    # Use JMESPath to filter results\n",
    "    page_iterator = paginator.paginate(\n",
    "        Bucket=bucket,\n",
    "        Prefix=prefix\n",
    "    ).search(f\"Contents[?Size > `{min_size_bytes}`]\")\n",
    "    \n",
    "    for obj in page_iterator:\n",
    "        if obj:  # JMESPath can return None\n",
    "            yield {\n",
    "                'key': obj['Key'],\n",
    "                'size_mb': obj['Size'] / (1024 * 1024)\n",
    "            }\n",
    "\n",
    "print(\"JMESPath filtering example defined\")\n",
    "print(\"\"\"\n",
    "Usage:\n",
    "for obj in list_large_objects('my-bucket', min_size_mb=100):\n",
    "    print(f\"{obj['key']}: {obj['size_mb']:.1f} MB\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DynamoDB pagination example\n",
    "def scan_dynamodb_table(table_name: str) -> Generator:\n",
    "    \"\"\"\n",
    "    Scan entire DynamoDB table using pagination.\n",
    "    \n",
    "    Warning: Full table scans are expensive - use queries when possible!\n",
    "    \"\"\"\n",
    "    dynamodb = boto3.client('dynamodb')\n",
    "    paginator = dynamodb.get_paginator('scan')\n",
    "    \n",
    "    for page in paginator.paginate(TableName=table_name):\n",
    "        for item in page.get('Items', []):\n",
    "            # Convert DynamoDB format to Python dict\n",
    "            yield deserialize_dynamodb_item(item)\n",
    "\n",
    "def deserialize_dynamodb_item(item: dict) -> dict:\n",
    "    \"\"\"Convert DynamoDB item format to regular Python dict.\"\"\"\n",
    "    from boto3.dynamodb.types import TypeDeserializer\n",
    "    deserializer = TypeDeserializer()\n",
    "    return {k: deserializer.deserialize(v) for k, v in item.items()}\n",
    "\n",
    "print(\"DynamoDB pagination example defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic paginator wrapper\n",
    "class AWSPaginator:\n",
    "    \"\"\"Generic wrapper for AWS pagination with progress tracking.\"\"\"\n",
    "    \n",
    "    def __init__(self, client, operation: str):\n",
    "        \"\"\"\n",
    "        Initialize paginator.\n",
    "        \n",
    "        Args:\n",
    "            client: Boto3 client\n",
    "            operation: API operation name\n",
    "        \"\"\"\n",
    "        self.paginator = client.get_paginator(operation)\n",
    "        self.total_items = 0\n",
    "        self.total_pages = 0\n",
    "    \n",
    "    def paginate(\n",
    "        self,\n",
    "        result_key: str,\n",
    "        max_items: int = None,\n",
    "        show_progress: bool = False,\n",
    "        **kwargs\n",
    "    ) -> Generator:\n",
    "        \"\"\"\n",
    "        Paginate through results.\n",
    "        \n",
    "        Args:\n",
    "            result_key: Key containing results in response\n",
    "            max_items: Maximum total items to return\n",
    "            show_progress: Print progress updates\n",
    "            **kwargs: Arguments passed to the paginate call\n",
    "        \"\"\"\n",
    "        config = {}\n",
    "        if max_items:\n",
    "            config['MaxItems'] = max_items\n",
    "        \n",
    "        if config:\n",
    "            kwargs['PaginationConfig'] = config\n",
    "        \n",
    "        for page in self.paginator.paginate(**kwargs):\n",
    "            self.total_pages += 1\n",
    "            items = page.get(result_key, [])\n",
    "            \n",
    "            for item in items:\n",
    "                self.total_items += 1\n",
    "                yield item\n",
    "                \n",
    "                if show_progress and self.total_items % 1000 == 0:\n",
    "                    print(f\"Processed {self.total_items} items...\")\n",
    "        \n",
    "        if show_progress:\n",
    "            print(f\"Complete: {self.total_items} items in {self.total_pages} pages\")\n",
    "\n",
    "print(\"AWSPaginator class defined\")\n",
    "print(\"\"\"\n",
    "Usage:\n",
    "s3 = boto3.client('s3')\n",
    "paginator = AWSPaginator(s3, 'list_objects_v2')\n",
    "\n",
    "for obj in paginator.paginate(\n",
    "    result_key='Contents',\n",
    "    Bucket='my-bucket',\n",
    "    show_progress=True\n",
    "):\n",
    "    process(obj)\n",
    "\n",
    "print(f\"Total: {paginator.total_items}\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Async Operations\n",
    "\n",
    "### Why Async?\n",
    "\n",
    "- **Concurrent requests**: Process multiple items in parallel\n",
    "- **Better throughput**: Don't wait for each request to complete\n",
    "- **Resource efficiency**: Handle I/O-bound operations efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Callable, Any\n",
    "\n",
    "class AsyncAWSClient:\n",
    "    \"\"\"\n",
    "    Async wrapper for boto3 operations using ThreadPoolExecutor.\n",
    "    \n",
    "    Note: boto3 is not truly async, but we can use threads\n",
    "    to achieve concurrency for I/O-bound operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_workers: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize async client.\n",
    "        \n",
    "        Args:\n",
    "            max_workers: Maximum concurrent operations\n",
    "        \"\"\"\n",
    "        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n",
    "    \n",
    "    async def run_async(self, func: Callable, *args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Run a synchronous function asynchronously.\n",
    "        \n",
    "        Args:\n",
    "            func: Function to run\n",
    "            *args, **kwargs: Arguments for the function\n",
    "            \n",
    "        Returns:\n",
    "            Function result\n",
    "        \"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(\n",
    "            self.executor,\n",
    "            lambda: func(*args, **kwargs)\n",
    "        )\n",
    "    \n",
    "    async def batch_operation(\n",
    "        self,\n",
    "        func: Callable,\n",
    "        items: List[Any],\n",
    "        batch_size: int = 10\n",
    "    ) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Execute operation on multiple items concurrently.\n",
    "        \n",
    "        Args:\n",
    "            func: Function to apply to each item\n",
    "            items: List of items to process\n",
    "            batch_size: Number of concurrent operations\n",
    "            \n",
    "        Returns:\n",
    "            List of results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(items), batch_size):\n",
    "            batch = items[i:i + batch_size]\n",
    "            tasks = [self.run_async(func, item) for item in batch]\n",
    "            batch_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            results.extend(batch_results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Shutdown the executor.\"\"\"\n",
    "        self.executor.shutdown(wait=True)\n",
    "\n",
    "print(\"AsyncAWSClient defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Async S3 downloads\n",
    "async def download_files_async(bucket: str, keys: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Download multiple S3 files concurrently.\n",
    "    \n",
    "    Args:\n",
    "        bucket: S3 bucket name\n",
    "        keys: List of object keys to download\n",
    "        \n",
    "    Returns:\n",
    "        List of download results\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    client = AsyncAWSClient(max_workers=10)\n",
    "    \n",
    "    def download_one(key: str) -> Dict:\n",
    "        \"\"\"Download a single file.\"\"\"\n",
    "        try:\n",
    "            response = s3.get_object(Bucket=bucket, Key=key)\n",
    "            content = response['Body'].read()\n",
    "            return {\n",
    "                'key': key,\n",
    "                'success': True,\n",
    "                'size': len(content),\n",
    "                'content': content\n",
    "            }\n",
    "        except ClientError as e:\n",
    "            return {\n",
    "                'key': key,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    results = await client.batch_operation(download_one, keys, batch_size=10)\n",
    "    client.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Async S3 download example defined\")\n",
    "print(\"\"\"\n",
    "Usage:\n",
    "keys = ['file1.json', 'file2.json', 'file3.json']\n",
    "results = await download_files_async('my-bucket', keys)\n",
    "\n",
    "for result in results:\n",
    "    if result['success']:\n",
    "        print(f\"Downloaded {result['key']}: {result['size']} bytes\")\n",
    "    else:\n",
    "        print(f\"Failed {result['key']}: {result['error']}\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Async DynamoDB batch writes\n",
    "async def batch_write_dynamodb_async(\n",
    "    table_name: str,\n",
    "    items: List[Dict],\n",
    "    batch_size: int = 25\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Write items to DynamoDB in parallel batches.\n",
    "    \n",
    "    Args:\n",
    "        table_name: DynamoDB table name\n",
    "        items: Items to write\n",
    "        batch_size: Items per batch (max 25 for DynamoDB)\n",
    "        \n",
    "    Returns:\n",
    "        Statistics dictionary\n",
    "    \"\"\"\n",
    "    dynamodb = boto3.resource('dynamodb')\n",
    "    table = dynamodb.Table(table_name)\n",
    "    \n",
    "    stats = {'written': 0, 'failed': 0}\n",
    "    client = AsyncAWSClient(max_workers=5)\n",
    "    \n",
    "    def write_batch(batch_items: List[Dict]) -> Dict:\n",
    "        \"\"\"Write a batch of items.\"\"\"\n",
    "        try:\n",
    "            with table.batch_writer() as batch:\n",
    "                for item in batch_items:\n",
    "                    batch.put_item(Item=item)\n",
    "            return {'success': True, 'count': len(batch_items)}\n",
    "        except Exception as e:\n",
    "            return {'success': False, 'error': str(e)}\n",
    "    \n",
    "    # Split into batches\n",
    "    batches = [\n",
    "        items[i:i + batch_size]\n",
    "        for i in range(0, len(items), batch_size)\n",
    "    ]\n",
    "    \n",
    "    results = await client.batch_operation(write_batch, batches, batch_size=5)\n",
    "    client.close()\n",
    "    \n",
    "    for result in results:\n",
    "        if isinstance(result, dict) and result.get('success'):\n",
    "            stats['written'] += result['count']\n",
    "        else:\n",
    "            stats['failed'] += batch_size\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"Async DynamoDB batch write example defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Best Practices\n",
    "\n",
    "### Resource Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice 1: Connection pooling and client reuse\n",
    "from botocore.config import Config\n",
    "\n",
    "class AWSClientFactory:\n",
    "    \"\"\"\n",
    "    Factory for creating and reusing AWS clients.\n",
    "    \n",
    "    Creating clients is expensive - reuse them!\n",
    "    \"\"\"\n",
    "    \n",
    "    _clients = {}\n",
    "    _config = Config(\n",
    "        max_pool_connections=50,\n",
    "        retries={\n",
    "            'max_attempts': 3,\n",
    "            'mode': 'adaptive'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    @classmethod\n",
    "    def get_client(cls, service: str, region: str = 'us-east-1'):\n",
    "        \"\"\"\n",
    "        Get or create a client for a service.\n",
    "        \n",
    "        Args:\n",
    "            service: AWS service name\n",
    "            region: AWS region\n",
    "            \n",
    "        Returns:\n",
    "            Boto3 client\n",
    "        \"\"\"\n",
    "        key = f\"{service}:{region}\"\n",
    "        \n",
    "        if key not in cls._clients:\n",
    "            cls._clients[key] = boto3.client(\n",
    "                service,\n",
    "                region_name=region,\n",
    "                config=cls._config\n",
    "            )\n",
    "        \n",
    "        return cls._clients[key]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_resource(cls, service: str, region: str = 'us-east-1'):\n",
    "        \"\"\"Get or create a resource for a service.\"\"\"\n",
    "        key = f\"{service}:resource:{region}\"\n",
    "        \n",
    "        if key not in cls._clients:\n",
    "            cls._clients[key] = boto3.resource(\n",
    "                service,\n",
    "                region_name=region,\n",
    "                config=cls._config\n",
    "            )\n",
    "        \n",
    "        return cls._clients[key]\n",
    "\n",
    "print(\"AWSClientFactory defined\")\n",
    "print(\"\"\"\n",
    "# Good: Reuse clients\n",
    "s3 = AWSClientFactory.get_client('s3')\n",
    "dynamodb = AWSClientFactory.get_resource('dynamodb')\n",
    "\n",
    "# Bad: Creating new clients repeatedly\n",
    "for item in items:\n",
    "    s3 = boto3.client('s3')  # Don't do this!\n",
    "    s3.put_object(...)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice 2: Context managers for resources\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def s3_streaming_upload(bucket: str, key: str):\n",
    "    \"\"\"\n",
    "    Context manager for streaming uploads to S3.\n",
    "    \n",
    "    Handles multipart uploads automatically.\n",
    "    \"\"\"\n",
    "    import io\n",
    "    s3 = boto3.client('s3')\n",
    "    buffer = io.BytesIO()\n",
    "    \n",
    "    try:\n",
    "        yield buffer\n",
    "        buffer.seek(0)\n",
    "        s3.upload_fileobj(buffer, bucket, key)\n",
    "    finally:\n",
    "        buffer.close()\n",
    "\n",
    "print(\"S3 streaming upload context manager defined\")\n",
    "print(\"\"\"\n",
    "Usage:\n",
    "with s3_streaming_upload('my-bucket', 'output.csv') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in data:\n",
    "        writer.writerow(row)\n",
    "# Automatically uploads when context exits\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice 3: Structured logging\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class AWSOperationLogger:\n",
    "    \"\"\"Structured logging for AWS operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.logger = logging.getLogger(name)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Add handler if not exists\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            handler.setFormatter(logging.Formatter(\n",
    "                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "            ))\n",
    "            self.logger.addHandler(handler)\n",
    "    \n",
    "    def log_operation(\n",
    "        self,\n",
    "        operation: str,\n",
    "        service: str,\n",
    "        success: bool,\n",
    "        duration_ms: float = None,\n",
    "        **extra\n",
    "    ):\n",
    "        \"\"\"Log an AWS operation with structured data.\"\"\"\n",
    "        log_data = {\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'operation': operation,\n",
    "            'service': service,\n",
    "            'success': success,\n",
    "            **extra\n",
    "        }\n",
    "        \n",
    "        if duration_ms:\n",
    "            log_data['duration_ms'] = round(duration_ms, 2)\n",
    "        \n",
    "        level = logging.INFO if success else logging.ERROR\n",
    "        self.logger.log(level, json.dumps(log_data))\n",
    "    \n",
    "    def log_error(self, operation: str, service: str, error: Exception, **extra):\n",
    "        \"\"\"Log an error.\"\"\"\n",
    "        self.log_operation(\n",
    "            operation=operation,\n",
    "            service=service,\n",
    "            success=False,\n",
    "            error_type=type(error).__name__,\n",
    "            error_message=str(error),\n",
    "            **extra\n",
    "        )\n",
    "\n",
    "print(\"AWSOperationLogger defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice 4: Configuration management\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class AWSConfig:\n",
    "    \"\"\"Configuration for AWS operations.\"\"\"\n",
    "    \n",
    "    region: str = field(default_factory=lambda: os.environ.get('AWS_REGION', 'us-east-1'))\n",
    "    profile: Optional[str] = field(default_factory=lambda: os.environ.get('AWS_PROFILE'))\n",
    "    max_retries: int = 3\n",
    "    connect_timeout: int = 5\n",
    "    read_timeout: int = 30\n",
    "    max_pool_connections: int = 25\n",
    "    \n",
    "    # Service-specific settings\n",
    "    s3_bucket: Optional[str] = field(default_factory=lambda: os.environ.get('S3_BUCKET'))\n",
    "    dynamodb_table: Optional[str] = field(default_factory=lambda: os.environ.get('DYNAMODB_TABLE'))\n",
    "    sqs_queue_url: Optional[str] = field(default_factory=lambda: os.environ.get('SQS_QUEUE_URL'))\n",
    "    \n",
    "    def get_boto_config(self) -> Config:\n",
    "        \"\"\"Get botocore Config object.\"\"\"\n",
    "        return Config(\n",
    "            region_name=self.region,\n",
    "            retries={'max_attempts': self.max_retries, 'mode': 'adaptive'},\n",
    "            connect_timeout=self.connect_timeout,\n",
    "            read_timeout=self.read_timeout,\n",
    "            max_pool_connections=self.max_pool_connections\n",
    "        )\n",
    "    \n",
    "    def create_session(self) -> boto3.Session:\n",
    "        \"\"\"Create a boto3 session with this config.\"\"\"\n",
    "        kwargs = {'region_name': self.region}\n",
    "        if self.profile:\n",
    "            kwargs['profile_name'] = self.profile\n",
    "        return boto3.Session(**kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_env(cls) -> 'AWSConfig':\n",
    "        \"\"\"Create config from environment variables.\"\"\"\n",
    "        return cls(\n",
    "            region=os.environ.get('AWS_REGION', 'us-east-1'),\n",
    "            profile=os.environ.get('AWS_PROFILE'),\n",
    "            max_retries=int(os.environ.get('AWS_MAX_RETRIES', '3')),\n",
    "            s3_bucket=os.environ.get('S3_BUCKET'),\n",
    "            dynamodb_table=os.environ.get('DYNAMODB_TABLE'),\n",
    "            sqs_queue_url=os.environ.get('SQS_QUEUE_URL')\n",
    "        )\n",
    "\n",
    "print(\"AWSConfig class defined\")\n",
    "print(\"\"\"\n",
    "Usage:\n",
    "# From environment\n",
    "config = AWSConfig.from_env()\n",
    "\n",
    "# Create session with config\n",
    "session = config.create_session()\n",
    "s3 = session.client('s3', config=config.get_boto_config())\n",
    "\n",
    "# Use configured values\n",
    "s3.list_objects_v2(Bucket=config.s3_bucket)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice 5: Health checks and monitoring\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "@dataclass\n",
    "class HealthCheckResult:\n",
    "    service: str\n",
    "    healthy: bool\n",
    "    latency_ms: float\n",
    "    message: str = \"\"\n",
    "\n",
    "class AWSHealthChecker:\n",
    "    \"\"\"Check health of AWS services.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: AWSConfig = None):\n",
    "        self.config = config or AWSConfig()\n",
    "        self.session = self.config.create_session()\n",
    "    \n",
    "    def check_s3(self, bucket: str = None) -> HealthCheckResult:\n",
    "        \"\"\"Check S3 connectivity.\"\"\"\n",
    "        bucket = bucket or self.config.s3_bucket\n",
    "        s3 = self.session.client('s3')\n",
    "        \n",
    "        start = time.time()\n",
    "        try:\n",
    "            s3.head_bucket(Bucket=bucket)\n",
    "            latency = (time.time() - start) * 1000\n",
    "            return HealthCheckResult('s3', True, latency, f\"Bucket {bucket} accessible\")\n",
    "        except ClientError as e:\n",
    "            latency = (time.time() - start) * 1000\n",
    "            return HealthCheckResult('s3', False, latency, str(e))\n",
    "    \n",
    "    def check_dynamodb(self, table: str = None) -> HealthCheckResult:\n",
    "        \"\"\"Check DynamoDB connectivity.\"\"\"\n",
    "        table = table or self.config.dynamodb_table\n",
    "        dynamodb = self.session.client('dynamodb')\n",
    "        \n",
    "        start = time.time()\n",
    "        try:\n",
    "            dynamodb.describe_table(TableName=table)\n",
    "            latency = (time.time() - start) * 1000\n",
    "            return HealthCheckResult('dynamodb', True, latency, f\"Table {table} accessible\")\n",
    "        except ClientError as e:\n",
    "            latency = (time.time() - start) * 1000\n",
    "            return HealthCheckResult('dynamodb', False, latency, str(e))\n",
    "    \n",
    "    def check_sqs(self, queue_url: str = None) -> HealthCheckResult:\n",
    "        \"\"\"Check SQS connectivity.\"\"\"\n",
    "        queue_url = queue_url or self.config.sqs_queue_url\n",
    "        sqs = self.session.client('sqs')\n",
    "        \n",
    "        start = time.time()\n",
    "        try:\n",
    "            sqs.get_queue_attributes(\n",
    "                QueueUrl=queue_url,\n",
    "                AttributeNames=['ApproximateNumberOfMessages']\n",
    "            )\n",
    "            latency = (time.time() - start) * 1000\n",
    "            return HealthCheckResult('sqs', True, latency, \"Queue accessible\")\n",
    "        except ClientError as e:\n",
    "            latency = (time.time() - start) * 1000\n",
    "            return HealthCheckResult('sqs', False, latency, str(e))\n",
    "    \n",
    "    def check_all(self) -> Dict[str, HealthCheckResult]:\n",
    "        \"\"\"Check all configured services.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        if self.config.s3_bucket:\n",
    "            results['s3'] = self.check_s3()\n",
    "        \n",
    "        if self.config.dynamodb_table:\n",
    "            results['dynamodb'] = self.check_dynamodb()\n",
    "        \n",
    "        if self.config.sqs_queue_url:\n",
    "            results['sqs'] = self.check_sqs()\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"AWSHealthChecker class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement a Retry Handler\n",
    "\n",
    "Create a comprehensive retry handler that handles different types of AWS errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "from typing import Callable, Any, Optional\n",
    "\n",
    "class SmartRetryHandler:\n",
    "    \"\"\"Intelligent retry handler for AWS operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):\n",
    "        pass\n",
    "    \n",
    "    def execute(self, func: Callable, *args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Execute function with smart retry logic.\n",
    "        \n",
    "        Should handle:\n",
    "        - Throttling errors (with longer backoff)\n",
    "        - Transient errors (with standard backoff)\n",
    "        - Non-retryable errors (fail immediately)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "import time\n",
    "import random\n",
    "from typing import Callable, Any, Set, Dict\n",
    "from botocore.exceptions import ClientError\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SmartRetryHandler:\n",
    "    \"\"\"Intelligent retry handler for AWS operations.\"\"\"\n",
    "    \n",
    "    # Errors that indicate throttling - use longer backoff\n",
    "    THROTTLING_ERRORS: Set[str] = {\n",
    "        'Throttling',\n",
    "        'ThrottlingException',\n",
    "        'ThrottledException',\n",
    "        'ProvisionedThroughputExceededException',\n",
    "        'RequestLimitExceeded',\n",
    "        'TooManyRequestsException',\n",
    "        'SlowDown',\n",
    "    }\n",
    "    \n",
    "    # Transient errors that can be retried\n",
    "    TRANSIENT_ERRORS: Set[str] = {\n",
    "        'ServiceException',\n",
    "        'ServiceUnavailable',\n",
    "        'InternalError',\n",
    "        'InternalServerError',\n",
    "        'RequestTimeout',\n",
    "        'EC2ThrottledException',\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        max_retries: int = 3,\n",
    "        base_delay: float = 1.0,\n",
    "        max_delay: float = 60.0,\n",
    "        throttle_multiplier: float = 2.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize retry handler.\n",
    "        \n",
    "        Args:\n",
    "            max_retries: Maximum retry attempts\n",
    "            base_delay: Base delay in seconds\n",
    "            max_delay: Maximum delay cap\n",
    "            throttle_multiplier: Extra multiplier for throttling errors\n",
    "        \"\"\"\n",
    "        self.max_retries = max_retries\n",
    "        self.base_delay = base_delay\n",
    "        self.max_delay = max_delay\n",
    "        self.throttle_multiplier = throttle_multiplier\n",
    "        self._stats = {\n",
    "            'attempts': 0,\n",
    "            'retries': 0,\n",
    "            'throttles': 0,\n",
    "            'failures': 0\n",
    "        }\n",
    "    \n",
    "    def _get_error_code(self, error: ClientError) -> str:\n",
    "        \"\"\"Extract error code from ClientError.\"\"\"\n",
    "        return error.response.get('Error', {}).get('Code', 'Unknown')\n",
    "    \n",
    "    def _is_retryable(self, error: ClientError) -> bool:\n",
    "        \"\"\"Check if error should be retried.\"\"\"\n",
    "        code = self._get_error_code(error)\n",
    "        return code in self.THROTTLING_ERRORS or code in self.TRANSIENT_ERRORS\n",
    "    \n",
    "    def _is_throttling(self, error: ClientError) -> bool:\n",
    "        \"\"\"Check if error is a throttling error.\"\"\"\n",
    "        return self._get_error_code(error) in self.THROTTLING_ERRORS\n",
    "    \n",
    "    def _calculate_delay(self, attempt: int, is_throttle: bool) -> float:\n",
    "        \"\"\"\n",
    "        Calculate delay with exponential backoff and jitter.\n",
    "        \n",
    "        Args:\n",
    "            attempt: Current attempt number (0-indexed)\n",
    "            is_throttle: Whether this is a throttling error\n",
    "        \"\"\"\n",
    "        # Exponential backoff\n",
    "        delay = self.base_delay * (2 ** attempt)\n",
    "        \n",
    "        # Extra delay for throttling\n",
    "        if is_throttle:\n",
    "            delay *= self.throttle_multiplier\n",
    "        \n",
    "        # Add jitter (0-25%)\n",
    "        jitter = delay * random.uniform(0, 0.25)\n",
    "        delay += jitter\n",
    "        \n",
    "        # Cap at max delay\n",
    "        return min(delay, self.max_delay)\n",
    "    \n",
    "    def execute(self, func: Callable, *args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Execute function with smart retry logic.\n",
    "        \n",
    "        Args:\n",
    "            func: Function to execute\n",
    "            *args, **kwargs: Arguments to pass to function\n",
    "            \n",
    "        Returns:\n",
    "            Function result\n",
    "            \n",
    "        Raises:\n",
    "            Last exception if all retries fail\n",
    "        \"\"\"\n",
    "        last_error = None\n",
    "        \n",
    "        for attempt in range(self.max_retries + 1):\n",
    "            self._stats['attempts'] += 1\n",
    "            \n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "                \n",
    "            except ClientError as e:\n",
    "                last_error = e\n",
    "                error_code = self._get_error_code(e)\n",
    "                \n",
    "                # Check if retryable\n",
    "                if not self._is_retryable(e):\n",
    "                    logger.error(f\"Non-retryable error: {error_code}\")\n",
    "                    self._stats['failures'] += 1\n",
    "                    raise\n",
    "                \n",
    "                # Check if we have retries left\n",
    "                if attempt >= self.max_retries:\n",
    "                    logger.error(f\"Max retries exceeded for {error_code}\")\n",
    "                    self._stats['failures'] += 1\n",
    "                    raise\n",
    "                \n",
    "                # Track throttling\n",
    "                is_throttle = self._is_throttling(e)\n",
    "                if is_throttle:\n",
    "                    self._stats['throttles'] += 1\n",
    "                \n",
    "                # Calculate and apply delay\n",
    "                delay = self._calculate_delay(attempt, is_throttle)\n",
    "                \n",
    "                logger.warning(\n",
    "                    f\"Attempt {attempt + 1} failed with {error_code}, \"\n",
    "                    f\"retrying in {delay:.2f}s \"\n",
    "                    f\"({'throttled' if is_throttle else 'transient'})\"\n",
    "                )\n",
    "                \n",
    "                self._stats['retries'] += 1\n",
    "                time.sleep(delay)\n",
    "        \n",
    "        raise last_error\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"Get retry statistics.\"\"\"\n",
    "        return self._stats.copy()\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        \"\"\"Reset statistics.\"\"\"\n",
    "        self._stats = {\n",
    "            'attempts': 0,\n",
    "            'retries': 0,\n",
    "            'throttles': 0,\n",
    "            'failures': 0\n",
    "        }\n",
    "\n",
    "# Usage example\n",
    "retry_handler = SmartRetryHandler(max_retries=5)\n",
    "\n",
    "def get_item(table_name, key):\n",
    "    dynamodb = boto3.client('dynamodb')\n",
    "    return dynamodb.get_item(TableName=table_name, Key=key)\n",
    "\n",
    "# result = retry_handler.execute(get_item, 'my-table', {'id': {'S': '123'}})\n",
    "print(f\"Stats: {retry_handler.get_stats()}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement a Paginated Iterator\n",
    "\n",
    "Create a generic iterator that handles pagination for any AWS service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "from typing import Iterator, Any, Callable\n",
    "\n",
    "class GenericAWSIterator:\n",
    "    \"\"\"Generic paginated iterator for AWS services.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        client,\n",
    "        operation: str,\n",
    "        result_key: str,\n",
    "        **kwargs\n",
    "    ):\n",
    "        pass\n",
    "    \n",
    "    def __iter__(self) -> Iterator[Any]:\n",
    "        pass\n",
    "    \n",
    "    def filter(self, predicate: Callable[[Any], bool]) -> 'GenericAWSIterator':\n",
    "        \"\"\"Add client-side filter.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def limit(self, n: int) -> 'GenericAWSIterator':\n",
    "        \"\"\"Limit results.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "from typing import Iterator, Any, Callable, Optional, List\n",
    "import boto3\n",
    "\n",
    "class GenericAWSIterator:\n",
    "    \"\"\"\n",
    "    Generic paginated iterator for AWS services.\n",
    "    \n",
    "    Supports filtering, limiting, and chaining operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        client,\n",
    "        operation: str,\n",
    "        result_key: str,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize iterator.\n",
    "        \n",
    "        Args:\n",
    "            client: Boto3 client\n",
    "            operation: Paginator operation name\n",
    "            result_key: Key containing results in response\n",
    "            **kwargs: Arguments to pass to paginate()\n",
    "        \"\"\"\n",
    "        self.client = client\n",
    "        self.operation = operation\n",
    "        self.result_key = result_key\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "        self._filters: List[Callable[[Any], bool]] = []\n",
    "        self._limit: Optional[int] = None\n",
    "        self._transform: Optional[Callable[[Any], Any]] = None\n",
    "        self._count = 0\n",
    "    \n",
    "    def __iter__(self) -> Iterator[Any]:\n",
    "        \"\"\"Iterate through paginated results.\"\"\"\n",
    "        self._count = 0\n",
    "        paginator = self.client.get_paginator(self.operation)\n",
    "        \n",
    "        for page in paginator.paginate(**self.kwargs):\n",
    "            items = page.get(self.result_key, [])\n",
    "            \n",
    "            for item in items:\n",
    "                # Check limit\n",
    "                if self._limit is not None and self._count >= self._limit:\n",
    "                    return\n",
    "                \n",
    "                # Apply filters\n",
    "                if all(f(item) for f in self._filters):\n",
    "                    self._count += 1\n",
    "                    \n",
    "                    # Apply transform if set\n",
    "                    if self._transform:\n",
    "                        yield self._transform(item)\n",
    "                    else:\n",
    "                        yield item\n",
    "    \n",
    "    def filter(self, predicate: Callable[[Any], bool]) -> 'GenericAWSIterator':\n",
    "        \"\"\"\n",
    "        Add a filter predicate.\n",
    "        \n",
    "        Args:\n",
    "            predicate: Function that returns True to keep item\n",
    "            \n",
    "        Returns:\n",
    "            Self for chaining\n",
    "        \"\"\"\n",
    "        self._filters.append(predicate)\n",
    "        return self\n",
    "    \n",
    "    def limit(self, n: int) -> 'GenericAWSIterator':\n",
    "        \"\"\"\n",
    "        Limit number of results.\n",
    "        \n",
    "        Args:\n",
    "            n: Maximum number of results\n",
    "            \n",
    "        Returns:\n",
    "            Self for chaining\n",
    "        \"\"\"\n",
    "        self._limit = n\n",
    "        return self\n",
    "    \n",
    "    def transform(self, func: Callable[[Any], Any]) -> 'GenericAWSIterator':\n",
    "        \"\"\"\n",
    "        Transform each result.\n",
    "        \n",
    "        Args:\n",
    "            func: Transform function\n",
    "            \n",
    "        Returns:\n",
    "            Self for chaining\n",
    "        \"\"\"\n",
    "        self._transform = func\n",
    "        return self\n",
    "    \n",
    "    def to_list(self) -> List[Any]:\n",
    "        \"\"\"Collect all results into a list.\"\"\"\n",
    "        return list(self)\n",
    "    \n",
    "    def first(self) -> Optional[Any]:\n",
    "        \"\"\"Get first result or None.\"\"\"\n",
    "        for item in self:\n",
    "            return item\n",
    "        return None\n",
    "    \n",
    "    def count(self) -> int:\n",
    "        \"\"\"Count all matching results.\"\"\"\n",
    "        return sum(1 for _ in self)\n",
    "    \n",
    "    @property\n",
    "    def items_yielded(self) -> int:\n",
    "        \"\"\"Number of items yielded so far.\"\"\"\n",
    "        return self._count\n",
    "\n",
    "# Example usage\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List large files in a bucket\n",
    "large_files = (\n",
    "    GenericAWSIterator(s3, 'list_objects_v2', 'Contents', Bucket='my-bucket')\n",
    "    .filter(lambda obj: obj['Size'] > 1024 * 1024)  # > 1MB\n",
    "    .transform(lambda obj: {'key': obj['Key'], 'size_mb': obj['Size'] / (1024*1024)})\n",
    "    .limit(100)\n",
    ")\n",
    "\n",
    "# for file in large_files:\n",
    "#     print(f\"{file['key']}: {file['size_mb']:.1f} MB\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: AWS Service Health Dashboard\n",
    "\n",
    "Create a comprehensive health dashboard for multiple AWS services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "@dataclass\n",
    "class ServiceHealth:\n",
    "    name: str\n",
    "    status: str  # 'healthy', 'degraded', 'unhealthy'\n",
    "    latency_ms: float\n",
    "    details: Dict[str, Any]\n",
    "\n",
    "class AWSDashboard:\n",
    "    \"\"\"AWS service health dashboard.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict):\n",
    "        pass\n",
    "    \n",
    "    def check_all_services(self) -> List[ServiceHealth]:\n",
    "        \"\"\"Check all configured services.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get overall health summary.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "from botocore.exceptions import ClientError\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "@dataclass\n",
    "class ServiceHealth:\n",
    "    name: str\n",
    "    status: str  # 'healthy', 'degraded', 'unhealthy'\n",
    "    latency_ms: float\n",
    "    details: Dict[str, Any] = field(default_factory=dict)\n",
    "    checked_at: datetime = field(default_factory=datetime.utcnow)\n",
    "    error: Optional[str] = None\n",
    "\n",
    "class AWSDashboard:\n",
    "    \"\"\"Comprehensive AWS service health dashboard.\"\"\"\n",
    "    \n",
    "    # Latency thresholds (ms)\n",
    "    LATENCY_HEALTHY = 200\n",
    "    LATENCY_DEGRADED = 1000\n",
    "    \n",
    "    def __init__(self, config: dict, region: str = 'us-east-1'):\n",
    "        \"\"\"\n",
    "        Initialize dashboard.\n",
    "        \n",
    "        Args:\n",
    "            config: Service configuration dict\n",
    "                {\n",
    "                    's3': {'bucket': 'my-bucket'},\n",
    "                    'dynamodb': {'table': 'my-table'},\n",
    "                    'sqs': {'queue_url': '...'},\n",
    "                    'lambda': {'function': 'my-function'}\n",
    "                }\n",
    "            region: AWS region\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.region = region\n",
    "        self.session = boto3.Session(region_name=region)\n",
    "        self._last_check: Optional[datetime] = None\n",
    "        self._last_results: List[ServiceHealth] = []\n",
    "    \n",
    "    def _check_s3(self, bucket: str) -> ServiceHealth:\n",
    "        \"\"\"Check S3 bucket health.\"\"\"\n",
    "        s3 = self.session.client('s3')\n",
    "        start = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Check bucket exists\n",
    "            s3.head_bucket(Bucket=bucket)\n",
    "            \n",
    "            # Get bucket stats\n",
    "            objects = s3.list_objects_v2(Bucket=bucket, MaxKeys=1)\n",
    "            \n",
    "            latency = (time.time() - start) * 1000\n",
    "            status = self._latency_to_status(latency)\n",
    "            \n",
    "            return ServiceHealth(\n",
    "                name='s3',\n",
    "                status=status,\n",
    "                latency_ms=latency,\n",
    "                details={\n",
    "                    'bucket': bucket,\n",
    "                    'has_objects': objects.get('KeyCount', 0) > 0\n",
    "                }\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            latency = (time.time() - start) * 1000\n",
    "            return ServiceHealth(\n",
    "                name='s3',\n",
    "                status='unhealthy',\n",
    "                latency_ms=latency,\n",
    "                error=str(e),\n",
    "                details={'bucket': bucket}\n",
    "            )\n",
    "    \n",
    "    def _check_dynamodb(self, table: str) -> ServiceHealth:\n",
    "        \"\"\"Check DynamoDB table health.\"\"\"\n",
    "        dynamodb = self.session.client('dynamodb')\n",
    "        start = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = dynamodb.describe_table(TableName=table)\n",
    "            table_info = response['Table']\n",
    "            \n",
    "            latency = (time.time() - start) * 1000\n",
    "            \n",
    "            # Check table status\n",
    "            table_status = table_info['TableStatus']\n",
    "            if table_status != 'ACTIVE':\n",
    "                status = 'degraded'\n",
    "            else:\n",
    "                status = self._latency_to_status(latency)\n",
    "            \n",
    "            return ServiceHealth(\n",
    "                name='dynamodb',\n",
    "                status=status,\n",
    "                latency_ms=latency,\n",
    "                details={\n",
    "                    'table': table,\n",
    "                    'table_status': table_status,\n",
    "                    'item_count': table_info.get('ItemCount', 0),\n",
    "                    'size_bytes': table_info.get('TableSizeBytes', 0)\n",
    "                }\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            latency = (time.time() - start) * 1000\n",
    "            return ServiceHealth(\n",
    "                name='dynamodb',\n",
    "                status='unhealthy',\n",
    "                latency_ms=latency,\n",
    "                error=str(e),\n",
    "                details={'table': table}\n",
    "            )\n",
    "    \n",
    "    def _check_sqs(self, queue_url: str) -> ServiceHealth:\n",
    "        \"\"\"Check SQS queue health.\"\"\"\n",
    "        sqs = self.session.client('sqs')\n",
    "        start = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = sqs.get_queue_attributes(\n",
    "                QueueUrl=queue_url,\n",
    "                AttributeNames=['All']\n",
    "            )\n",
    "            attrs = response['Attributes']\n",
    "            \n",
    "            latency = (time.time() - start) * 1000\n",
    "            status = self._latency_to_status(latency)\n",
    "            \n",
    "            return ServiceHealth(\n",
    "                name='sqs',\n",
    "                status=status,\n",
    "                latency_ms=latency,\n",
    "                details={\n",
    "                    'queue_url': queue_url,\n",
    "                    'messages_available': int(attrs.get('ApproximateNumberOfMessages', 0)),\n",
    "                    'messages_in_flight': int(attrs.get('ApproximateNumberOfMessagesNotVisible', 0)),\n",
    "                    'messages_delayed': int(attrs.get('ApproximateNumberOfMessagesDelayed', 0))\n",
    "                }\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            latency = (time.time() - start) * 1000\n",
    "            return ServiceHealth(\n",
    "                name='sqs',\n",
    "                status='unhealthy',\n",
    "                latency_ms=latency,\n",
    "                error=str(e)\n",
    "            )\n",
    "    \n",
    "    def _check_lambda(self, function_name: str) -> ServiceHealth:\n",
    "        \"\"\"Check Lambda function health.\"\"\"\n",
    "        lambda_client = self.session.client('lambda')\n",
    "        start = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = lambda_client.get_function(FunctionName=function_name)\n",
    "            config = response['Configuration']\n",
    "            \n",
    "            latency = (time.time() - start) * 1000\n",
    "            \n",
    "            func_state = config.get('State', 'Unknown')\n",
    "            if func_state != 'Active':\n",
    "                status = 'degraded'\n",
    "            else:\n",
    "                status = self._latency_to_status(latency)\n",
    "            \n",
    "            return ServiceHealth(\n",
    "                name='lambda',\n",
    "                status=status,\n",
    "                latency_ms=latency,\n",
    "                details={\n",
    "                    'function': function_name,\n",
    "                    'state': func_state,\n",
    "                    'runtime': config.get('Runtime'),\n",
    "                    'memory_mb': config.get('MemorySize'),\n",
    "                    'timeout': config.get('Timeout'),\n",
    "                    'last_modified': config.get('LastModified')\n",
    "                }\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            latency = (time.time() - start) * 1000\n",
    "            return ServiceHealth(\n",
    "                name='lambda',\n",
    "                status='unhealthy',\n",
    "                latency_ms=latency,\n",
    "                error=str(e),\n",
    "                details={'function': function_name}\n",
    "            )\n",
    "    \n",
    "    def _latency_to_status(self, latency_ms: float) -> str:\n",
    "        \"\"\"Convert latency to status.\"\"\"\n",
    "        if latency_ms < self.LATENCY_HEALTHY:\n",
    "            return 'healthy'\n",
    "        elif latency_ms < self.LATENCY_DEGRADED:\n",
    "            return 'degraded'\n",
    "        return 'unhealthy'\n",
    "    \n",
    "    def check_all_services(self, parallel: bool = True) -> List[ServiceHealth]:\n",
    "        \"\"\"\n",
    "        Check all configured services.\n",
    "        \n",
    "        Args:\n",
    "            parallel: Run checks in parallel\n",
    "            \n",
    "        Returns:\n",
    "            List of health check results\n",
    "        \"\"\"\n",
    "        checks = []\n",
    "        \n",
    "        if 's3' in self.config:\n",
    "            checks.append(('s3', self._check_s3, self.config['s3']['bucket']))\n",
    "        \n",
    "        if 'dynamodb' in self.config:\n",
    "            checks.append(('dynamodb', self._check_dynamodb, self.config['dynamodb']['table']))\n",
    "        \n",
    "        if 'sqs' in self.config:\n",
    "            checks.append(('sqs', self._check_sqs, self.config['sqs']['queue_url']))\n",
    "        \n",
    "        if 'lambda' in self.config:\n",
    "            checks.append(('lambda', self._check_lambda, self.config['lambda']['function']))\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        if parallel:\n",
    "            with ThreadPoolExecutor(max_workers=len(checks)) as executor:\n",
    "                futures = {\n",
    "                    executor.submit(func, arg): name\n",
    "                    for name, func, arg in checks\n",
    "                }\n",
    "                for future in as_completed(futures):\n",
    "                    results.append(future.result())\n",
    "        else:\n",
    "            for name, func, arg in checks:\n",
    "                results.append(func(arg))\n",
    "        \n",
    "        self._last_check = datetime.utcnow()\n",
    "        self._last_results = results\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get overall health summary.\"\"\"\n",
    "        if not self._last_results:\n",
    "            self.check_all_services()\n",
    "        \n",
    "        healthy = sum(1 for r in self._last_results if r.status == 'healthy')\n",
    "        degraded = sum(1 for r in self._last_results if r.status == 'degraded')\n",
    "        unhealthy = sum(1 for r in self._last_results if r.status == 'unhealthy')\n",
    "        \n",
    "        avg_latency = sum(r.latency_ms for r in self._last_results) / len(self._last_results)\n",
    "        \n",
    "        # Overall status\n",
    "        if unhealthy > 0:\n",
    "            overall = 'unhealthy'\n",
    "        elif degraded > 0:\n",
    "            overall = 'degraded'\n",
    "        else:\n",
    "            overall = 'healthy'\n",
    "        \n",
    "        return {\n",
    "            'overall_status': overall,\n",
    "            'checked_at': self._last_check.isoformat() if self._last_check else None,\n",
    "            'services': {\n",
    "                'total': len(self._last_results),\n",
    "                'healthy': healthy,\n",
    "                'degraded': degraded,\n",
    "                'unhealthy': unhealthy\n",
    "            },\n",
    "            'average_latency_ms': round(avg_latency, 2),\n",
    "            'details': [\n",
    "                {\n",
    "                    'service': r.name,\n",
    "                    'status': r.status,\n",
    "                    'latency_ms': round(r.latency_ms, 2),\n",
    "                    'error': r.error\n",
    "                }\n",
    "                for r in self._last_results\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "config = {\n",
    "    's3': {'bucket': 'my-bucket'},\n",
    "    'dynamodb': {'table': 'my-table'},\n",
    "    'sqs': {'queue_url': 'https://sqs.us-east-1.amazonaws.com/123456789/my-queue'},\n",
    "    'lambda': {'function': 'my-function'}\n",
    "}\n",
    "\n",
    "dashboard = AWSDashboard(config)\n",
    "# summary = dashboard.get_summary()\n",
    "# print(json.dumps(summary, indent=2))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. **Error Handling**\n",
    "   - Botocore exception hierarchy\n",
    "   - Retryable vs non-retryable errors\n",
    "   - Exponential backoff with jitter\n",
    "\n",
    "2. **Pagination**\n",
    "   - Manual token-based pagination\n",
    "   - Using boto3 paginators\n",
    "   - JMESPath filtering\n",
    "\n",
    "3. **Async Operations**\n",
    "   - ThreadPoolExecutor for concurrency\n",
    "   - Batch processing patterns\n",
    "   - Parallel downloads/uploads\n",
    "\n",
    "4. **Best Practices**\n",
    "   - Client reuse and pooling\n",
    "   - Configuration management\n",
    "   - Structured logging\n",
    "   - Health monitoring\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Always handle errors**: AWS operations can fail\n",
    "- **Use paginators**: Don't assume one request gets all data\n",
    "- **Reuse clients**: Creating clients is expensive\n",
    "- **Implement retries**: Transient errors are normal\n",
    "- **Monitor health**: Track latency and availability\n",
    "\n",
    "### AWS Cost Warning\n",
    "\n",
    "> **Production Considerations**:\n",
    "> - Retries increase API call count (and cost)\n",
    "> - Pagination may require many requests for large datasets\n",
    "> - Use appropriate timeouts to avoid hung operations\n",
    "> - Consider reserved capacity for predictable workloads\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This module has covered the essential patterns for working with AWS using boto3:\n",
    "\n",
    "1. **01_aws_fundamentals_setup**: Account basics, IAM, credentials\n",
    "2. **02_s3_operations**: Storage operations\n",
    "3. **03_other_services**: DynamoDB, Lambda, SQS, SNS\n",
    "4. **04_practical_patterns**: Error handling, pagination, best practices\n",
    "\n",
    "You now have the knowledge to build robust, production-ready AWS applications with Python!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
