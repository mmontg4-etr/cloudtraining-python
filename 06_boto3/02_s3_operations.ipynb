{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon S3 Operations with Boto3\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Create, list, and delete S3 buckets\n",
    "2. Upload and download files to/from S3\n",
    "3. List and filter objects in a bucket\n",
    "4. Generate presigned URLs for temporary access\n",
    "5. Manage object metadata and tags\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. S3 Fundamentals\n",
    "\n",
    "### What is Amazon S3?\n",
    "\n",
    "Amazon Simple Storage Service (S3) is an object storage service that offers:\n",
    "- **Durability**: 99.999999999% (11 9's)\n",
    "- **Availability**: 99.99%\n",
    "- **Scalability**: Virtually unlimited storage\n",
    "- **Security**: Encryption, access policies, versioning\n",
    "\n",
    "### S3 Concepts\n",
    "\n",
    "| Concept | Description | Example |\n",
    "|---------|-------------|---------|\n",
    "| **Bucket** | Container for objects | `my-company-data` |\n",
    "| **Object** | File + metadata | `images/logo.png` |\n",
    "| **Key** | Unique object identifier | `folder/subfolder/file.txt` |\n",
    "| **Region** | Geographic location | `us-east-1` |\n",
    "\n",
    "### S3 URL Formats\n",
    "\n",
    "```\n",
    "Virtual-hosted style (preferred):\n",
    "  https://bucket-name.s3.region.amazonaws.com/key\n",
    "  https://my-bucket.s3.us-east-1.amazonaws.com/images/photo.jpg\n",
    "\n",
    "Path style (legacy):\n",
    "  https://s3.region.amazonaws.com/bucket-name/key\n",
    "  https://s3.us-east-1.amazonaws.com/my-bucket/images/photo.jpg\n",
    "\n",
    "S3 URI:\n",
    "  s3://bucket-name/key\n",
    "  s3://my-bucket/images/photo.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import libraries and create clients\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError, NoCredentialsError\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "# Create S3 client and resource\n",
    "# Note: These will fail without AWS credentials - that's expected\n",
    "try:\n",
    "    s3_client = boto3.client('s3', region_name='us-east-1')\n",
    "    s3_resource = boto3.resource('s3', region_name='us-east-1')\n",
    "    print(\"S3 client and resource created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create S3 client: {e}\")\n",
    "    print(\"Continuing with examples - they will show code patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bucket Operations\n",
    "\n",
    "### Bucket Naming Rules\n",
    "\n",
    "- 3-63 characters long\n",
    "- Lowercase letters, numbers, and hyphens only\n",
    "- Must start with a letter or number\n",
    "- **Globally unique** across ALL AWS accounts\n",
    "- Cannot be formatted as an IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def validate_bucket_name(name: str) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate S3 bucket name according to AWS rules.\n",
    "    \n",
    "    Args:\n",
    "        name: Proposed bucket name\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_valid, message)\n",
    "    \"\"\"\n",
    "    # Check length\n",
    "    if len(name) < 3 or len(name) > 63:\n",
    "        return False, \"Name must be 3-63 characters long\"\n",
    "    \n",
    "    # Check for lowercase and valid characters\n",
    "    if not re.match(r'^[a-z0-9][a-z0-9.-]*[a-z0-9]$', name):\n",
    "        return False, \"Must start/end with letter or number, contain only lowercase, numbers, hyphens, periods\"\n",
    "    \n",
    "    # Check for consecutive periods\n",
    "    if '..' in name:\n",
    "        return False, \"Cannot contain consecutive periods\"\n",
    "    \n",
    "    # Check for IP address format\n",
    "    if re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', name):\n",
    "        return False, \"Cannot be formatted as IP address\"\n",
    "    \n",
    "    return True, \"Valid bucket name\"\n",
    "\n",
    "# Test bucket names\n",
    "test_names = [\n",
    "    \"my-bucket\",\n",
    "    \"My-Bucket\",  # Invalid - uppercase\n",
    "    \"ab\",          # Invalid - too short\n",
    "    \"my..bucket\",  # Invalid - consecutive periods\n",
    "    \"192.168.1.1\", # Invalid - IP address\n",
    "    \"my-company-data-2024\",\n",
    "]\n",
    "\n",
    "print(\"Bucket Name Validation:\")\n",
    "print(\"=\" * 50)\n",
    "for name in test_names:\n",
    "    is_valid, message = validate_bucket_name(name)\n",
    "    status = \"VALID\" if is_valid else \"INVALID\"\n",
    "    print(f\"{name:25} -> {status}: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(\n",
    "    bucket_name: str,\n",
    "    region: str = 'us-east-1',\n",
    "    enable_versioning: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create an S3 bucket with optional versioning.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket to create\n",
    "        region: AWS region for the bucket\n",
    "        enable_versioning: Whether to enable versioning\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with creation result\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3', region_name=region)\n",
    "    \n",
    "    try:\n",
    "        # Note: us-east-1 doesn't require LocationConstraint\n",
    "        if region == 'us-east-1':\n",
    "            response = s3.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            response = s3.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={\n",
    "                    'LocationConstraint': region\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Enable versioning if requested\n",
    "        if enable_versioning:\n",
    "            s3.put_bucket_versioning(\n",
    "                Bucket=bucket_name,\n",
    "                VersioningConfiguration={'Status': 'Enabled'}\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'bucket_name': bucket_name,\n",
    "            'region': region,\n",
    "            'versioning': enable_versioning,\n",
    "            'location': response.get('Location')\n",
    "        }\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        error_message = e.response['Error']['Message']\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error_code': error_code,\n",
    "            'error_message': error_message\n",
    "        }\n",
    "\n",
    "# Example usage (would require real AWS credentials)\n",
    "print(\"Example: Creating a bucket\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\"\"\n",
    "result = create_bucket(\n",
    "    bucket_name='my-unique-bucket-12345',\n",
    "    region='us-west-2',\n",
    "    enable_versioning=True\n",
    ")\n",
    "print(result)\n",
    "\n",
    "# Expected output:\n",
    "# {\n",
    "#     'success': True,\n",
    "#     'bucket_name': 'my-unique-bucket-12345',\n",
    "#     'region': 'us-west-2',\n",
    "#     'versioning': True,\n",
    "#     'location': '/my-unique-bucket-12345'\n",
    "# }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_buckets(s3_client=None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    List all S3 buckets in the account.\n",
    "    \n",
    "    Returns:\n",
    "        List of bucket information dictionaries\n",
    "    \"\"\"\n",
    "    if s3_client is None:\n",
    "        s3_client = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        response = s3_client.list_buckets()\n",
    "        \n",
    "        buckets = []\n",
    "        for bucket in response['Buckets']:\n",
    "            # Get bucket region\n",
    "            try:\n",
    "                location = s3_client.get_bucket_location(Bucket=bucket['Name'])\n",
    "                region = location['LocationConstraint'] or 'us-east-1'\n",
    "            except ClientError:\n",
    "                region = 'unknown'\n",
    "            \n",
    "            buckets.append({\n",
    "                'name': bucket['Name'],\n",
    "                'created': bucket['CreationDate'].isoformat(),\n",
    "                'region': region\n",
    "            })\n",
    "        \n",
    "        return buckets\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"Error listing buckets: {e}\")\n",
    "        return []\n",
    "\n",
    "# Demonstration with mock data\n",
    "print(\"Example: List buckets output\")\n",
    "print(\"-\" * 40)\n",
    "mock_buckets = [\n",
    "    {'name': 'my-app-data', 'created': '2024-01-15T10:30:00', 'region': 'us-east-1'},\n",
    "    {'name': 'my-app-backups', 'created': '2024-02-20T14:45:00', 'region': 'us-west-2'},\n",
    "    {'name': 'my-app-logs', 'created': '2024-03-10T09:00:00', 'region': 'eu-west-1'},\n",
    "]\n",
    "\n",
    "for bucket in mock_buckets:\n",
    "    print(f\"Bucket: {bucket['name']}\")\n",
    "    print(f\"  Region: {bucket['region']}\")\n",
    "    print(f\"  Created: {bucket['created']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_bucket(bucket_name: str, force: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Delete an S3 bucket.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the bucket to delete\n",
    "        force: If True, delete all objects first\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with deletion result\n",
    "    \"\"\"\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    \n",
    "    try:\n",
    "        if force:\n",
    "            # Delete all objects (including versions)\n",
    "            bucket.object_versions.delete()\n",
    "            print(f\"Deleted all objects from {bucket_name}\")\n",
    "        \n",
    "        # Delete the bucket\n",
    "        bucket.delete()\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'bucket_name': bucket_name,\n",
    "            'message': 'Bucket deleted successfully'\n",
    "        }\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        \n",
    "        if error_code == 'BucketNotEmpty':\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error_code': error_code,\n",
    "                'message': 'Bucket is not empty. Use force=True to delete all objects first.'\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'success': False,\n",
    "            'error_code': error_code,\n",
    "            'message': e.response['Error']['Message']\n",
    "        }\n",
    "\n",
    "print(\"Example: Deleting a bucket\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\"\"\n",
    "# Delete empty bucket\n",
    "result = delete_bucket('my-empty-bucket')\n",
    "\n",
    "# Force delete (removes all objects first)\n",
    "result = delete_bucket('my-bucket-with-data', force=True)\n",
    "\n",
    "# WARNING: force=True will permanently delete all data!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Object Operations\n",
    "\n",
    "### Uploading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def upload_file(\n",
    "    file_path: str,\n",
    "    bucket_name: str,\n",
    "    object_key: Optional[str] = None,\n",
    "    metadata: Optional[Dict[str, str]] = None,\n",
    "    tags: Optional[Dict[str, str]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Upload a file to S3.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Local path to the file\n",
    "        bucket_name: Target bucket name\n",
    "        object_key: S3 key (default: filename)\n",
    "        metadata: Custom metadata dictionary\n",
    "        tags: Tags dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with upload result\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Use filename as key if not specified\n",
    "    if object_key is None:\n",
    "        object_key = Path(file_path).name\n",
    "    \n",
    "    # Build extra args\n",
    "    extra_args = {}\n",
    "    if metadata:\n",
    "        extra_args['Metadata'] = metadata\n",
    "    if tags:\n",
    "        tag_string = '&'.join([f\"{k}={v}\" for k, v in tags.items()])\n",
    "        extra_args['Tagging'] = tag_string\n",
    "    \n",
    "    try:\n",
    "        s3.upload_file(\n",
    "            Filename=file_path,\n",
    "            Bucket=bucket_name,\n",
    "            Key=object_key,\n",
    "            ExtraArgs=extra_args if extra_args else None\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'bucket': bucket_name,\n",
    "            'key': object_key,\n",
    "            's3_uri': f's3://{bucket_name}/{object_key}'\n",
    "        }\n",
    "        \n",
    "    except ClientError as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "    except FileNotFoundError:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': f'File not found: {file_path}'\n",
    "        }\n",
    "\n",
    "print(\"Example: Upload file\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\"\"\n",
    "result = upload_file(\n",
    "    file_path='./data/report.pdf',\n",
    "    bucket_name='my-bucket',\n",
    "    object_key='reports/2024/report.pdf',\n",
    "    metadata={'author': 'John Doe', 'department': 'Finance'},\n",
    "    tags={'project': 'quarterly-reports', 'year': '2024'}\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# {\n",
    "#     'success': True,\n",
    "#     'bucket': 'my-bucket',\n",
    "#     'key': 'reports/2024/report.pdf',\n",
    "#     's3_uri': 's3://my-bucket/reports/2024/report.pdf'\n",
    "# }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload with progress callback\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "class UploadProgressCallback:\n",
    "    \"\"\"Callback class to track upload progress.\"\"\"\n",
    "    \n",
    "    def __init__(self, filename: str):\n",
    "        self._filename = filename\n",
    "        self._size = os.path.getsize(filename)\n",
    "        self._seen_so_far = 0\n",
    "        self._lock = threading.Lock()\n",
    "    \n",
    "    def __call__(self, bytes_amount: int):\n",
    "        with self._lock:\n",
    "            self._seen_so_far += bytes_amount\n",
    "            percentage = (self._seen_so_far / self._size) * 100\n",
    "            sys.stdout.write(\n",
    "                f\"\\r{self._filename}: {self._seen_so_far}/{self._size} bytes \"\n",
    "                f\"({percentage:.1f}%)\"\n",
    "            )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "def upload_with_progress(file_path: str, bucket: str, key: str):\n",
    "    \"\"\"\n",
    "    Upload a file with progress tracking.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Local file path\n",
    "        bucket: S3 bucket name\n",
    "        key: S3 object key\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    callback = UploadProgressCallback(file_path)\n",
    "    s3.upload_file(file_path, bucket, key, Callback=callback)\n",
    "    print()  # New line after progress\n",
    "\n",
    "print(\"Upload with progress tracking:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\"\"\n",
    "upload_with_progress(\n",
    "    file_path='./large_file.zip',\n",
    "    bucket='my-bucket',\n",
    "    key='uploads/large_file.zip'\n",
    ")\n",
    "\n",
    "# Output:\n",
    "# large_file.zip: 52428800/104857600 bytes (50.0%)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Data Directly (Without Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def upload_data(\n",
    "    data: bytes | str,\n",
    "    bucket_name: str,\n",
    "    object_key: str,\n",
    "    content_type: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Upload data directly to S3 without a local file.\n",
    "    \n",
    "    Args:\n",
    "        data: Bytes or string data to upload\n",
    "        bucket_name: Target bucket\n",
    "        object_key: S3 key\n",
    "        content_type: MIME type of the content\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with upload result\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Convert string to bytes if necessary\n",
    "    if isinstance(data, str):\n",
    "        data = data.encode('utf-8')\n",
    "    \n",
    "    try:\n",
    "        extra_args = {}\n",
    "        if content_type:\n",
    "            extra_args['ContentType'] = content_type\n",
    "        \n",
    "        s3.put_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=object_key,\n",
    "            Body=data,\n",
    "            **extra_args\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'bucket': bucket_name,\n",
    "            'key': object_key,\n",
    "            'size': len(data)\n",
    "        }\n",
    "        \n",
    "    except ClientError as e:\n",
    "        return {'success': False, 'error': str(e)}\n",
    "\n",
    "# Example: Upload JSON data\n",
    "print(\"Example: Upload JSON directly\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\"\"\n",
    "config = {\n",
    "    'app_name': 'MyApp',\n",
    "    'version': '1.0.0',\n",
    "    'settings': {'debug': False, 'log_level': 'INFO'}\n",
    "}\n",
    "\n",
    "result = upload_data(\n",
    "    data=json.dumps(config, indent=2),\n",
    "    bucket_name='my-bucket',\n",
    "    object_key='config/app_config.json',\n",
    "    content_type='application/json'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(\n",
    "    bucket_name: str,\n",
    "    object_key: str,\n",
    "    local_path: str\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Download a file from S3.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Source bucket\n",
    "        object_key: S3 object key\n",
    "        local_path: Local file path to save to\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with download result\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Create directory if needed\n",
    "    Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        s3.download_file(bucket_name, object_key, local_path)\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'local_path': local_path,\n",
    "            'size': os.path.getsize(local_path)\n",
    "        }\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            return {'success': False, 'error': 'Object not found'}\n",
    "        return {'success': False, 'error': str(e)}\n",
    "\n",
    "def download_to_memory(bucket_name: str, object_key: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Download S3 object directly to memory.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Source bucket\n",
    "        object_key: S3 object key\n",
    "        \n",
    "    Returns:\n",
    "        Object content as bytes\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    response = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    return response['Body'].read()\n",
    "\n",
    "print(\"Example: Download files\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\"\"\n",
    "# Download to file\n",
    "result = download_file(\n",
    "    bucket_name='my-bucket',\n",
    "    object_key='reports/2024/report.pdf',\n",
    "    local_path='./downloads/report.pdf'\n",
    ")\n",
    "\n",
    "# Download to memory\n",
    "data = download_to_memory('my-bucket', 'config/app_config.json')\n",
    "config = json.loads(data)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_objects(\n",
    "    bucket_name: str,\n",
    "    prefix: str = '',\n",
    "    max_keys: int = 1000\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    List objects in an S3 bucket.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Bucket to list\n",
    "        prefix: Filter by prefix (folder path)\n",
    "        max_keys: Maximum number of keys to return\n",
    "        \n",
    "    Returns:\n",
    "        List of object information dictionaries\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    objects = []\n",
    "    \n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    \n",
    "    for page in paginator.paginate(\n",
    "        Bucket=bucket_name,\n",
    "        Prefix=prefix,\n",
    "        PaginationConfig={'MaxItems': max_keys}\n",
    "    ):\n",
    "        for obj in page.get('Contents', []):\n",
    "            objects.append({\n",
    "                'key': obj['Key'],\n",
    "                'size': obj['Size'],\n",
    "                'last_modified': obj['LastModified'].isoformat(),\n",
    "                'storage_class': obj.get('StorageClass', 'STANDARD')\n",
    "            })\n",
    "    \n",
    "    return objects\n",
    "\n",
    "# Mock example output\n",
    "print(\"Example: List objects output\")\n",
    "print(\"-\" * 40)\n",
    "mock_objects = [\n",
    "    {'key': 'data/users.json', 'size': 1024, 'last_modified': '2024-03-15T10:00:00', 'storage_class': 'STANDARD'},\n",
    "    {'key': 'data/products.json', 'size': 2048, 'last_modified': '2024-03-15T11:00:00', 'storage_class': 'STANDARD'},\n",
    "    {'key': 'backups/db_backup.sql', 'size': 5242880, 'last_modified': '2024-03-14T00:00:00', 'storage_class': 'GLACIER'},\n",
    "]\n",
    "\n",
    "for obj in mock_objects:\n",
    "    size_kb = obj['size'] / 1024\n",
    "    print(f\"{obj['key']:30} {size_kb:>10.1f} KB  {obj['storage_class']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_folders(bucket_name: str, prefix: str = '') -> List[str]:\n",
    "    \"\"\"\n",
    "    List 'folders' (common prefixes) in an S3 bucket.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Bucket to list\n",
    "        prefix: Parent prefix to list folders under\n",
    "        \n",
    "    Returns:\n",
    "        List of folder prefixes\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    response = s3.list_objects_v2(\n",
    "        Bucket=bucket_name,\n",
    "        Prefix=prefix,\n",
    "        Delimiter='/'  # This groups results by 'folder'\n",
    "    )\n",
    "    \n",
    "    folders = []\n",
    "    for cp in response.get('CommonPrefixes', []):\n",
    "        folders.append(cp['Prefix'])\n",
    "    \n",
    "    return folders\n",
    "\n",
    "print(\"Example: List folders\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\"\"\n",
    "# Bucket structure:\n",
    "# my-bucket/\n",
    "#   data/\n",
    "#     users.json\n",
    "#     products.json\n",
    "#   backups/\n",
    "#     db_backup.sql\n",
    "#   logs/\n",
    "#     2024/\n",
    "#       app.log\n",
    "\n",
    "folders = list_folders('my-bucket')\n",
    "# Result: ['data/', 'backups/', 'logs/']\n",
    "\n",
    "folders = list_folders('my-bucket', prefix='logs/')\n",
    "# Result: ['logs/2024/']\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Presigned URLs\n",
    "\n",
    "Presigned URLs allow temporary access to private S3 objects without sharing credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_presigned_url(\n",
    "    bucket_name: str,\n",
    "    object_key: str,\n",
    "    expiration: int = 3600,\n",
    "    operation: str = 'get_object'\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Generate a presigned URL for S3 operations.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: S3 bucket name\n",
    "        object_key: S3 object key\n",
    "        expiration: URL expiration time in seconds (default: 1 hour)\n",
    "        operation: S3 operation ('get_object' or 'put_object')\n",
    "        \n",
    "    Returns:\n",
    "        Presigned URL or None if error\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        url = s3.generate_presigned_url(\n",
    "            operation,\n",
    "            Params={\n",
    "                'Bucket': bucket_name,\n",
    "                'Key': object_key\n",
    "            },\n",
    "            ExpiresIn=expiration\n",
    "        )\n",
    "        return url\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"Error generating presigned URL: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Example: Presigned URLs\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\"\"\n",
    "# Generate download URL (valid for 1 hour)\n",
    "download_url = generate_presigned_url(\n",
    "    bucket_name='my-bucket',\n",
    "    object_key='reports/confidential.pdf',\n",
    "    expiration=3600  # 1 hour\n",
    ")\n",
    "\n",
    "# Result:\n",
    "# https://my-bucket.s3.amazonaws.com/reports/confidential.pdf?\n",
    "#   X-Amz-Algorithm=AWS4-HMAC-SHA256&\n",
    "#   X-Amz-Credential=...&\n",
    "#   X-Amz-Date=...&\n",
    "#   X-Amz-Expires=3600&\n",
    "#   X-Amz-Signature=...\n",
    "\n",
    "# Users can download using this URL without AWS credentials\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_presigned_upload_url(\n",
    "    bucket_name: str,\n",
    "    object_key: str,\n",
    "    expiration: int = 3600,\n",
    "    content_type: Optional[str] = None,\n",
    "    max_size: Optional[int] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a presigned POST URL for uploads with conditions.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: S3 bucket name\n",
    "        object_key: S3 object key\n",
    "        expiration: URL expiration time in seconds\n",
    "        content_type: Required content type\n",
    "        max_size: Maximum file size in bytes\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with URL and fields for form upload\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    conditions = []\n",
    "    fields = {}\n",
    "    \n",
    "    if content_type:\n",
    "        conditions.append({'Content-Type': content_type})\n",
    "        fields['Content-Type'] = content_type\n",
    "    \n",
    "    if max_size:\n",
    "        conditions.append(['content-length-range', 1, max_size])\n",
    "    \n",
    "    try:\n",
    "        response = s3.generate_presigned_post(\n",
    "            Bucket=bucket_name,\n",
    "            Key=object_key,\n",
    "            Fields=fields,\n",
    "            Conditions=conditions,\n",
    "            ExpiresIn=expiration\n",
    "        )\n",
    "        return response\n",
    "        \n",
    "    except ClientError as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "print(\"Example: Presigned upload URL\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\"\"\n",
    "# Generate upload URL (for user uploads)\n",
    "upload_data = generate_presigned_upload_url(\n",
    "    bucket_name='my-bucket',\n",
    "    object_key='user-uploads/${filename}',\n",
    "    expiration=300,  # 5 minutes\n",
    "    content_type='image/jpeg',\n",
    "    max_size=5*1024*1024  # 5 MB\n",
    ")\n",
    "\n",
    "# Use in HTML form:\n",
    "# <form action=\"{upload_data['url']}\" method=\"post\" enctype=\"multipart/form-data\">\n",
    "#     {for field, value in upload_data['fields'].items()}\n",
    "#         <input type=\"hidden\" name=\"{field}\" value=\"{value}\">\n",
    "#     {endfor}\n",
    "#     <input type=\"file\" name=\"file\">\n",
    "#     <input type=\"submit\" value=\"Upload\">\n",
    "# </form>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Object Metadata and Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_metadata(bucket_name: str, object_key: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get metadata for an S3 object.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: S3 bucket name\n",
    "        object_key: S3 object key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with object metadata\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        response = s3.head_object(Bucket=bucket_name, Key=object_key)\n",
    "        \n",
    "        return {\n",
    "            'content_type': response.get('ContentType'),\n",
    "            'content_length': response.get('ContentLength'),\n",
    "            'last_modified': response.get('LastModified'),\n",
    "            'etag': response.get('ETag'),\n",
    "            'metadata': response.get('Metadata', {}),\n",
    "            'storage_class': response.get('StorageClass', 'STANDARD')\n",
    "        }\n",
    "        \n",
    "    except ClientError as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "def set_object_tags(\n",
    "    bucket_name: str,\n",
    "    object_key: str,\n",
    "    tags: Dict[str, str]\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Set tags on an S3 object.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: S3 bucket name\n",
    "        object_key: S3 object key\n",
    "        tags: Dictionary of tags\n",
    "        \n",
    "    Returns:\n",
    "        True if successful\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    tag_set = [{'Key': k, 'Value': v} for k, v in tags.items()]\n",
    "    \n",
    "    try:\n",
    "        s3.put_object_tagging(\n",
    "            Bucket=bucket_name,\n",
    "            Key=object_key,\n",
    "            Tagging={'TagSet': tag_set}\n",
    "        )\n",
    "        return True\n",
    "    except ClientError:\n",
    "        return False\n",
    "\n",
    "print(\"Example: Object metadata and tags\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\"\"\n",
    "# Get metadata\n",
    "metadata = get_object_metadata('my-bucket', 'data/users.json')\n",
    "# {\n",
    "#     'content_type': 'application/json',\n",
    "#     'content_length': 1024,\n",
    "#     'last_modified': datetime(2024, 3, 15, 10, 0, 0),\n",
    "#     'etag': '\"abc123...\"',\n",
    "#     'metadata': {'author': 'John', 'version': '1.0'},\n",
    "#     'storage_class': 'STANDARD'\n",
    "# }\n",
    "\n",
    "# Set tags\n",
    "set_object_tags(\n",
    "    'my-bucket',\n",
    "    'data/users.json',\n",
    "    {'environment': 'production', 'data-type': 'pii', 'retention': '7-years'}\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: S3 File Manager Class\n",
    "\n",
    "Create a class that provides a simple interface for common S3 operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "class S3FileManager:\n",
    "    \"\"\"Simple interface for S3 file operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name: str, region: str = 'us-east-1'):\n",
    "        \"\"\"Initialize the file manager.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def upload(self, local_path: str, s3_path: str) -> bool:\n",
    "        \"\"\"Upload a file.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def download(self, s3_path: str, local_path: str) -> bool:\n",
    "        \"\"\"Download a file.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def list(self, prefix: str = '') -> List[str]:\n",
    "        \"\"\"List files with optional prefix.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def delete(self, s3_path: str) -> bool:\n",
    "        \"\"\"Delete a file.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def exists(self, s3_path: str) -> bool:\n",
    "        \"\"\"Check if a file exists.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Test your class\n",
    "# manager = S3FileManager('my-bucket')\n",
    "# manager.upload('./data.json', 'uploads/data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "class S3FileManager:\n",
    "    \"\"\"Simple interface for S3 file operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name: str, region: str = 'us-east-1'):\n",
    "        \"\"\"\n",
    "        Initialize the file manager.\n",
    "        \n",
    "        Args:\n",
    "            bucket_name: S3 bucket to manage\n",
    "            region: AWS region\n",
    "        \"\"\"\n",
    "        self.bucket_name = bucket_name\n",
    "        self.region = region\n",
    "        self.client = boto3.client('s3', region_name=region)\n",
    "        self.resource = boto3.resource('s3', region_name=region)\n",
    "        self.bucket = self.resource.Bucket(bucket_name)\n",
    "    \n",
    "    def upload(self, local_path: str, s3_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Upload a file to S3.\n",
    "        \n",
    "        Args:\n",
    "            local_path: Local file path\n",
    "            s3_path: S3 object key\n",
    "            \n",
    "        Returns:\n",
    "            True if successful\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.client.upload_file(local_path, self.bucket_name, s3_path)\n",
    "            return True\n",
    "        except (ClientError, FileNotFoundError) as e:\n",
    "            print(f\"Upload failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def download(self, s3_path: str, local_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Download a file from S3.\n",
    "        \n",
    "        Args:\n",
    "            s3_path: S3 object key\n",
    "            local_path: Local file path\n",
    "            \n",
    "        Returns:\n",
    "            True if successful\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create directory if needed\n",
    "            Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "            self.client.download_file(self.bucket_name, s3_path, local_path)\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            print(f\"Download failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def list(self, prefix: str = '') -> List[str]:\n",
    "        \"\"\"\n",
    "        List files with optional prefix.\n",
    "        \n",
    "        Args:\n",
    "            prefix: S3 prefix to filter by\n",
    "            \n",
    "        Returns:\n",
    "            List of object keys\n",
    "        \"\"\"\n",
    "        keys = []\n",
    "        for obj in self.bucket.objects.filter(Prefix=prefix):\n",
    "            keys.append(obj.key)\n",
    "        return keys\n",
    "    \n",
    "    def delete(self, s3_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Delete a file from S3.\n",
    "        \n",
    "        Args:\n",
    "            s3_path: S3 object key\n",
    "            \n",
    "        Returns:\n",
    "            True if successful\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.client.delete_object(Bucket=self.bucket_name, Key=s3_path)\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            print(f\"Delete failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def exists(self, s3_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a file exists in S3.\n",
    "        \n",
    "        Args:\n",
    "            s3_path: S3 object key\n",
    "            \n",
    "        Returns:\n",
    "            True if exists\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.client.head_object(Bucket=self.bucket_name, Key=s3_path)\n",
    "            return True\n",
    "        except ClientError:\n",
    "            return False\n",
    "    \n",
    "    def get_url(self, s3_path: str, expiration: int = 3600) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get a presigned URL for a file.\n",
    "        \n",
    "        Args:\n",
    "            s3_path: S3 object key\n",
    "            expiration: URL expiration in seconds\n",
    "            \n",
    "        Returns:\n",
    "            Presigned URL\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.client.generate_presigned_url(\n",
    "                'get_object',\n",
    "                Params={'Bucket': self.bucket_name, 'Key': s3_path},\n",
    "                ExpiresIn=expiration\n",
    "            )\n",
    "        except ClientError:\n",
    "            return None\n",
    "\n",
    "# Example usage\n",
    "manager = S3FileManager('my-bucket', 'us-west-2')\n",
    "print(f\"Managing bucket: {manager.bucket_name}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Sync Local Directory to S3\n",
    "\n",
    "Create a function that syncs a local directory to an S3 prefix (like `aws s3 sync`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "import hashlib\n",
    "\n",
    "def sync_to_s3(\n",
    "    local_dir: str,\n",
    "    bucket_name: str,\n",
    "    s3_prefix: str = '',\n",
    "    delete: bool = False\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Sync a local directory to S3.\n",
    "    \n",
    "    Args:\n",
    "        local_dir: Local directory path\n",
    "        bucket_name: S3 bucket name\n",
    "        s3_prefix: S3 prefix (folder)\n",
    "        delete: If True, delete S3 objects not in local\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'uploaded', 'skipped', 'deleted' lists\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "# result = sync_to_s3('./my_data', 'my-bucket', 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def sync_to_s3(\n",
    "    local_dir: str,\n",
    "    bucket_name: str,\n",
    "    s3_prefix: str = '',\n",
    "    delete: bool = False\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Sync a local directory to S3.\n",
    "    \n",
    "    Args:\n",
    "        local_dir: Local directory path\n",
    "        bucket_name: S3 bucket name\n",
    "        s3_prefix: S3 prefix (folder)\n",
    "        delete: If True, delete S3 objects not in local\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'uploaded', 'skipped', 'deleted' lists\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    local_path = Path(local_dir)\n",
    "    \n",
    "    result = {\n",
    "        'uploaded': [],\n",
    "        'skipped': [],\n",
    "        'deleted': []\n",
    "    }\n",
    "    \n",
    "    def get_md5(file_path: str) -> str:\n",
    "        \"\"\"Calculate MD5 hash of a file.\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, 'rb') as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b''):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def get_s3_etag(bucket: str, key: str) -> str:\n",
    "        \"\"\"Get ETag of S3 object.\"\"\"\n",
    "        try:\n",
    "            response = s3.head_object(Bucket=bucket, Key=key)\n",
    "            # Remove quotes from ETag\n",
    "            return response['ETag'].strip('\"')\n",
    "        except ClientError:\n",
    "            return ''\n",
    "    \n",
    "    # Get all local files\n",
    "    local_files = {}\n",
    "    for file_path in local_path.rglob('*'):\n",
    "        if file_path.is_file():\n",
    "            relative_path = file_path.relative_to(local_path)\n",
    "            s3_key = f\"{s3_prefix}{relative_path}\".replace('\\\\', '/')\n",
    "            local_files[s3_key] = str(file_path)\n",
    "    \n",
    "    # Upload new or changed files\n",
    "    for s3_key, file_path in local_files.items():\n",
    "        local_md5 = get_md5(file_path)\n",
    "        s3_etag = get_s3_etag(bucket_name, s3_key)\n",
    "        \n",
    "        if local_md5 != s3_etag:\n",
    "            s3.upload_file(file_path, bucket_name, s3_key)\n",
    "            result['uploaded'].append(s3_key)\n",
    "        else:\n",
    "            result['skipped'].append(s3_key)\n",
    "    \n",
    "    # Delete files in S3 that don't exist locally\n",
    "    if delete:\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=bucket_name, Prefix=s3_prefix):\n",
    "            for obj in page.get('Contents', []):\n",
    "                if obj['Key'] not in local_files:\n",
    "                    s3.delete_object(Bucket=bucket_name, Key=obj['Key'])\n",
    "                    result['deleted'].append(obj['Key'])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "print(\"Syncing ./my_data to s3://my-bucket/data/\")\n",
    "# result = sync_to_s3('./my_data', 'my-bucket', 'data/', delete=True)\n",
    "# print(f\"Uploaded: {len(result['uploaded'])} files\")\n",
    "# print(f\"Skipped: {len(result['skipped'])} files\")\n",
    "# print(f\"Deleted: {len(result['deleted'])} files\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Batch Delete with Prefix\n",
    "\n",
    "Create a function to efficiently delete all objects matching a prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "\n",
    "def batch_delete(bucket_name: str, prefix: str, dry_run: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Delete all objects matching a prefix.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: S3 bucket\n",
    "        prefix: Prefix to match\n",
    "        dry_run: If True, only list what would be deleted\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with deletion results\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "# result = batch_delete('my-bucket', 'old-data/', dry_run=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "from typing import Dict, Any, List\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def batch_delete(bucket_name: str, prefix: str, dry_run: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Delete all objects matching a prefix.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: S3 bucket\n",
    "        prefix: Prefix to match\n",
    "        dry_run: If True, only list what would be deleted\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with deletion results\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    result = {\n",
    "        'dry_run': dry_run,\n",
    "        'bucket': bucket_name,\n",
    "        'prefix': prefix,\n",
    "        'objects_found': 0,\n",
    "        'objects_deleted': 0,\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    # Collect objects to delete\n",
    "    objects_to_delete = []\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    \n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "        for obj in page.get('Contents', []):\n",
    "            objects_to_delete.append({'Key': obj['Key']})\n",
    "    \n",
    "    result['objects_found'] = len(objects_to_delete)\n",
    "    \n",
    "    if dry_run:\n",
    "        print(f\"DRY RUN: Would delete {len(objects_to_delete)} objects\")\n",
    "        for obj in objects_to_delete[:10]:  # Show first 10\n",
    "            print(f\"  - {obj['Key']}\")\n",
    "        if len(objects_to_delete) > 10:\n",
    "            print(f\"  ... and {len(objects_to_delete) - 10} more\")\n",
    "        return result\n",
    "    \n",
    "    # Delete in batches of 1000 (S3 limit)\n",
    "    batch_size = 1000\n",
    "    \n",
    "    for i in range(0, len(objects_to_delete), batch_size):\n",
    "        batch = objects_to_delete[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            response = s3.delete_objects(\n",
    "                Bucket=bucket_name,\n",
    "                Delete={'Objects': batch}\n",
    "            )\n",
    "            \n",
    "            result['objects_deleted'] += len(response.get('Deleted', []))\n",
    "            \n",
    "            for error in response.get('Errors', []):\n",
    "                result['errors'].append({\n",
    "                    'key': error['Key'],\n",
    "                    'code': error['Code'],\n",
    "                    'message': error['Message']\n",
    "                })\n",
    "                \n",
    "        except ClientError as e:\n",
    "            result['errors'].append({'error': str(e)})\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example\n",
    "print(\"Batch delete example:\")\n",
    "# result = batch_delete('my-bucket', 'old-data/', dry_run=True)\n",
    "# print(f\"Found: {result['objects_found']} objects\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Generate Report of Bucket Usage\n",
    "\n",
    "Create a function that generates a usage report for a bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n",
    "\n",
    "def generate_bucket_report(bucket_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a usage report for an S3 bucket.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - total_objects: count\n",
    "        - total_size: bytes\n",
    "        - by_prefix: breakdown by top-level prefix\n",
    "        - by_extension: breakdown by file extension\n",
    "        - by_storage_class: breakdown by storage class\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_bucket_report(bucket_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a usage report for an S3 bucket.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with detailed bucket statistics\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    report = {\n",
    "        'bucket_name': bucket_name,\n",
    "        'total_objects': 0,\n",
    "        'total_size_bytes': 0,\n",
    "        'by_prefix': defaultdict(lambda: {'count': 0, 'size': 0}),\n",
    "        'by_extension': defaultdict(lambda: {'count': 0, 'size': 0}),\n",
    "        'by_storage_class': defaultdict(lambda: {'count': 0, 'size': 0}),\n",
    "    }\n",
    "    \n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    \n",
    "    for page in paginator.paginate(Bucket=bucket_name):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            size = obj['Size']\n",
    "            storage_class = obj.get('StorageClass', 'STANDARD')\n",
    "            \n",
    "            report['total_objects'] += 1\n",
    "            report['total_size_bytes'] += size\n",
    "            \n",
    "            # By prefix (top-level folder)\n",
    "            prefix = key.split('/')[0] if '/' in key else '(root)'\n",
    "            report['by_prefix'][prefix]['count'] += 1\n",
    "            report['by_prefix'][prefix]['size'] += size\n",
    "            \n",
    "            # By extension\n",
    "            ext = Path(key).suffix.lower() or '(no extension)'\n",
    "            report['by_extension'][ext]['count'] += 1\n",
    "            report['by_extension'][ext]['size'] += size\n",
    "            \n",
    "            # By storage class\n",
    "            report['by_storage_class'][storage_class]['count'] += 1\n",
    "            report['by_storage_class'][storage_class]['size'] += size\n",
    "    \n",
    "    # Convert defaultdicts to regular dicts\n",
    "    report['by_prefix'] = dict(report['by_prefix'])\n",
    "    report['by_extension'] = dict(report['by_extension'])\n",
    "    report['by_storage_class'] = dict(report['by_storage_class'])\n",
    "    \n",
    "    # Add human-readable size\n",
    "    def human_size(size_bytes):\n",
    "        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "            if size_bytes < 1024.0:\n",
    "                return f\"{size_bytes:.2f} {unit}\"\n",
    "            size_bytes /= 1024.0\n",
    "        return f\"{size_bytes:.2f} PB\"\n",
    "    \n",
    "    report['total_size_human'] = human_size(report['total_size_bytes'])\n",
    "    \n",
    "    return report\n",
    "\n",
    "def print_report(report: Dict[str, Any]):\n",
    "    \"\"\"Print a formatted bucket report.\"\"\"\n",
    "    print(f\"\\nBucket Report: {report['bucket_name']}\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total Objects: {report['total_objects']:,}\")\n",
    "    print(f\"Total Size: {report['total_size_human']}\")\n",
    "    \n",
    "    print(\"\\nBy Prefix:\")\n",
    "    for prefix, stats in sorted(report['by_prefix'].items(), \n",
    "                                 key=lambda x: x[1]['size'], reverse=True)[:5]:\n",
    "        print(f\"  {prefix:20} {stats['count']:>8,} files  {stats['size']:>15,} bytes\")\n",
    "    \n",
    "    print(\"\\nBy Extension:\")\n",
    "    for ext, stats in sorted(report['by_extension'].items(), \n",
    "                              key=lambda x: x[1]['size'], reverse=True)[:5]:\n",
    "        print(f\"  {ext:20} {stats['count']:>8,} files  {stats['size']:>15,} bytes\")\n",
    "    \n",
    "    print(\"\\nBy Storage Class:\")\n",
    "    for storage, stats in report['by_storage_class'].items():\n",
    "        print(f\"  {storage:20} {stats['count']:>8,} files  {stats['size']:>15,} bytes\")\n",
    "\n",
    "# Example\n",
    "# report = generate_bucket_report('my-bucket')\n",
    "# print_report(report)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Implement Copy Between Buckets\n",
    "\n",
    "Create a function to copy objects between buckets, optionally changing storage class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Your code here\n",
    "\n",
    "def copy_between_buckets(\n",
    "    source_bucket: str,\n",
    "    source_prefix: str,\n",
    "    dest_bucket: str,\n",
    "    dest_prefix: str,\n",
    "    storage_class: str = 'STANDARD'\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Copy objects between S3 buckets.\n",
    "    \n",
    "    Args:\n",
    "        source_bucket: Source bucket name\n",
    "        source_prefix: Source prefix\n",
    "        dest_bucket: Destination bucket name\n",
    "        dest_prefix: Destination prefix\n",
    "        storage_class: Storage class for copied objects\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with copy results\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see solution</summary>\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "from typing import Dict, Any\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def copy_between_buckets(\n",
    "    source_bucket: str,\n",
    "    source_prefix: str,\n",
    "    dest_bucket: str,\n",
    "    dest_prefix: str,\n",
    "    storage_class: str = 'STANDARD'\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Copy objects between S3 buckets.\n",
    "    \n",
    "    Args:\n",
    "        source_bucket: Source bucket name\n",
    "        source_prefix: Source prefix\n",
    "        dest_bucket: Destination bucket name\n",
    "        dest_prefix: Destination prefix\n",
    "        storage_class: Storage class for copied objects\n",
    "            Options: STANDARD, STANDARD_IA, ONEZONE_IA, \n",
    "                     INTELLIGENT_TIERING, GLACIER, DEEP_ARCHIVE\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with copy results\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    result = {\n",
    "        'copied': [],\n",
    "        'failed': [],\n",
    "        'total_size': 0\n",
    "    }\n",
    "    \n",
    "    # List source objects\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    \n",
    "    for page in paginator.paginate(Bucket=source_bucket, Prefix=source_prefix):\n",
    "        for obj in page.get('Contents', []):\n",
    "            source_key = obj['Key']\n",
    "            \n",
    "            # Calculate destination key\n",
    "            relative_key = source_key[len(source_prefix):]\n",
    "            dest_key = f\"{dest_prefix}{relative_key}\"\n",
    "            \n",
    "            try:\n",
    "                # Copy object\n",
    "                copy_source = {\n",
    "                    'Bucket': source_bucket,\n",
    "                    'Key': source_key\n",
    "                }\n",
    "                \n",
    "                s3.copy_object(\n",
    "                    CopySource=copy_source,\n",
    "                    Bucket=dest_bucket,\n",
    "                    Key=dest_key,\n",
    "                    StorageClass=storage_class\n",
    "                )\n",
    "                \n",
    "                result['copied'].append({\n",
    "                    'source': f\"{source_bucket}/{source_key}\",\n",
    "                    'destination': f\"{dest_bucket}/{dest_key}\",\n",
    "                    'size': obj['Size']\n",
    "                })\n",
    "                result['total_size'] += obj['Size']\n",
    "                \n",
    "            except ClientError as e:\n",
    "                result['failed'].append({\n",
    "                    'source': f\"{source_bucket}/{source_key}\",\n",
    "                    'error': str(e)\n",
    "                })\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "print(\"Copy between buckets example:\")\n",
    "print(\"\"\"\n",
    "result = copy_between_buckets(\n",
    "    source_bucket='production-data',\n",
    "    source_prefix='2024/01/',\n",
    "    dest_bucket='backup-bucket',\n",
    "    dest_prefix='archive/2024/01/',\n",
    "    storage_class='GLACIER'\n",
    ")\n",
    "\n",
    "print(f\"Copied: {len(result['copied'])} objects\")\n",
    "print(f\"Failed: {len(result['failed'])} objects\")\n",
    "print(f\"Total size: {result['total_size']} bytes\")\n",
    "\"\"\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. **S3 Fundamentals**\n",
    "   - Buckets, objects, and keys\n",
    "   - URL formats and naming rules\n",
    "\n",
    "2. **Bucket Operations**\n",
    "   - Creating buckets with versioning\n",
    "   - Listing and deleting buckets\n",
    "\n",
    "3. **Object Operations**\n",
    "   - Uploading files and data\n",
    "   - Downloading to file or memory\n",
    "   - Listing and filtering objects\n",
    "\n",
    "4. **Presigned URLs**\n",
    "   - Temporary download URLs\n",
    "   - Secure upload URLs\n",
    "\n",
    "5. **Metadata and Tags**\n",
    "   - Custom metadata\n",
    "   - Object tagging for management\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Use **presigned URLs** for secure temporary access\n",
    "- **Paginate** when listing large buckets\n",
    "- Use **batch operations** for deleting multiple objects\n",
    "- Consider **storage classes** for cost optimization\n",
    "- Always **validate bucket names** before creation\n",
    "\n",
    "### AWS Cost Warning\n",
    "\n",
    "> **S3 Pricing**: You pay for:\n",
    "> - Storage (per GB/month)\n",
    "> - Requests (PUT, GET, LIST, etc.)\n",
    "> - Data transfer OUT\n",
    "> \n",
    "> **Tips**:\n",
    "> - Use lifecycle policies to transition to cheaper storage\n",
    "> - Delete unused objects\n",
    "> - Use Intelligent-Tiering for unpredictable access patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to [03_other_services.ipynb](03_other_services.ipynb) to learn about other AWS services:\n",
    "- DynamoDB for NoSQL databases\n",
    "- Lambda for serverless computing\n",
    "- SQS and SNS for messaging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
