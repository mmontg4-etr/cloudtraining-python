{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project 3: Cloud Data Pipeline Concepts\n",
    "\n",
    "## Building Production-Ready Data Processing Patterns\n",
    "\n",
    "This capstone project demonstrates how to structure data pipelines that could be deployed to cloud environments. We'll build a complete ETL (Extract, Transform, Load) pipeline with:\n",
    "\n",
    "1. **Simulated cloud storage** (mimicking S3/Azure Blob/GCS patterns)\n",
    "2. **Data ingestion patterns** for various file formats\n",
    "3. **Data transformation** with Pandas\n",
    "4. **Data quality validation**\n",
    "5. **Results storage** with proper organization\n",
    "6. **Pipeline orchestration** patterns\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "In production environments, data pipelines need to:\n",
    "- Handle failures gracefully\n",
    "- Be testable and maintainable\n",
    "- Follow consistent patterns\n",
    "- Produce auditable outputs\n",
    "\n",
    "### Learning Objectives\n",
    "- Design modular, reusable pipeline components\n",
    "- Implement proper error handling and logging\n",
    "- Apply data validation patterns\n",
    "- Structure code for cloud deployment\n",
    "- Use configuration-driven processing\n",
    "\n",
    "**Note**: This notebook uses simulated cloud storage (local filesystem) to demonstrate patterns. The same code structure would work with real cloud SDKs (boto3, azure-storage, google-cloud-storage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Dict, List, Optional, Any, Callable, Protocol\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configure logging for pipeline visibility\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger('DataPipeline')\n",
    "\n",
    "print(\"Cloud Data Pipeline Framework Initialized\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Cloud Storage Abstraction Layer\n",
    "\n",
    "A key pattern in cloud development is abstracting storage operations. This allows:\n",
    "- Easy testing with local storage\n",
    "- Switching between cloud providers\n",
    "- Consistent interface across the codebase\n",
    "\n",
    "We'll implement a storage abstraction that mimics S3-style operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StorageBackend(Protocol):\n",
    "    \"\"\"\n",
    "    Protocol defining the interface for storage backends.\n",
    "    \n",
    "    This allows for dependency injection and easy swapping\n",
    "    between local and cloud storage implementations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def read(self, path: str) -> bytes:\n",
    "        \"\"\"Read raw bytes from storage.\"\"\"\n",
    "        ...\n",
    "    \n",
    "    def write(self, path: str, data: bytes) -> None:\n",
    "        \"\"\"Write raw bytes to storage.\"\"\"\n",
    "        ...\n",
    "    \n",
    "    def exists(self, path: str) -> bool:\n",
    "        \"\"\"Check if a path exists.\"\"\"\n",
    "        ...\n",
    "    \n",
    "    def list_objects(self, prefix: str) -> List[str]:\n",
    "        \"\"\"List objects with given prefix.\"\"\"\n",
    "        ...\n",
    "    \n",
    "    def delete(self, path: str) -> None:\n",
    "        \"\"\"Delete an object.\"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class LocalStorageBackend:\n",
    "    \"\"\"\n",
    "    Local filesystem storage backend that mimics cloud storage patterns.\n",
    "    \n",
    "    In production, this would be replaced with:\n",
    "    - S3StorageBackend (using boto3)\n",
    "    - AzureBlobBackend (using azure-storage-blob)\n",
    "    - GCSBackend (using google-cloud-storage)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    base_path : str\n",
    "        Base directory for storage (like an S3 bucket)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"Initialized local storage at: {self.base_path}\")\n",
    "    \n",
    "    def _full_path(self, path: str) -> Path:\n",
    "        \"\"\"Get full path for a storage key.\"\"\"\n",
    "        return self.base_path / path\n",
    "    \n",
    "    def read(self, path: str) -> bytes:\n",
    "        \"\"\"Read raw bytes from storage.\"\"\"\n",
    "        full_path = self._full_path(path)\n",
    "        if not full_path.exists():\n",
    "            raise FileNotFoundError(f\"Object not found: {path}\")\n",
    "        return full_path.read_bytes()\n",
    "    \n",
    "    def write(self, path: str, data: bytes) -> None:\n",
    "        \"\"\"Write raw bytes to storage.\"\"\"\n",
    "        full_path = self._full_path(path)\n",
    "        full_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        full_path.write_bytes(data)\n",
    "        logger.debug(f\"Written {len(data)} bytes to {path}\")\n",
    "    \n",
    "    def exists(self, path: str) -> bool:\n",
    "        \"\"\"Check if a path exists.\"\"\"\n",
    "        return self._full_path(path).exists()\n",
    "    \n",
    "    def list_objects(self, prefix: str = \"\") -> List[str]:\n",
    "        \"\"\"List all objects with given prefix.\"\"\"\n",
    "        search_path = self._full_path(prefix)\n",
    "        if search_path.is_file():\n",
    "            return [prefix]\n",
    "        \n",
    "        results = []\n",
    "        if search_path.exists():\n",
    "            for item in search_path.rglob('*'):\n",
    "                if item.is_file():\n",
    "                    results.append(str(item.relative_to(self.base_path)))\n",
    "        return sorted(results)\n",
    "    \n",
    "    def delete(self, path: str) -> None:\n",
    "        \"\"\"Delete an object.\"\"\"\n",
    "        full_path = self._full_path(path)\n",
    "        if full_path.exists():\n",
    "            full_path.unlink()\n",
    "            logger.debug(f\"Deleted: {path}\")\n",
    "\n",
    "\n",
    "class DataLake:\n",
    "    \"\"\"\n",
    "    High-level data lake interface supporting multiple formats.\n",
    "    \n",
    "    Provides convenient methods for reading/writing DataFrames\n",
    "    to various storage formats (CSV, Parquet, JSON).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    storage : StorageBackend\n",
    "        Backend storage implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage: StorageBackend):\n",
    "        self.storage = storage\n",
    "    \n",
    "    def read_csv(self, path: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Read CSV file into DataFrame.\"\"\"\n",
    "        data = self.storage.read(path)\n",
    "        from io import BytesIO\n",
    "        return pd.read_csv(BytesIO(data), **kwargs)\n",
    "    \n",
    "    def write_csv(self, df: pd.DataFrame, path: str, **kwargs) -> None:\n",
    "        \"\"\"Write DataFrame to CSV.\"\"\"\n",
    "        data = df.to_csv(index=False, **kwargs).encode('utf-8')\n",
    "        self.storage.write(path, data)\n",
    "        logger.info(f\"Written CSV: {path} ({len(df)} rows)\")\n",
    "    \n",
    "    def read_json(self, path: str, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Read JSON file into DataFrame.\"\"\"\n",
    "        data = self.storage.read(path)\n",
    "        from io import BytesIO\n",
    "        return pd.read_json(BytesIO(data), **kwargs)\n",
    "    \n",
    "    def write_json(self, df: pd.DataFrame, path: str, **kwargs) -> None:\n",
    "        \"\"\"Write DataFrame to JSON.\"\"\"\n",
    "        data = df.to_json(orient='records', **kwargs).encode('utf-8')\n",
    "        self.storage.write(path, data)\n",
    "        logger.info(f\"Written JSON: {path} ({len(df)} rows)\")\n",
    "    \n",
    "    def read_parquet(self, path: str) -> pd.DataFrame:\n",
    "        \"\"\"Read Parquet file into DataFrame.\"\"\"\n",
    "        # In production with actual Parquet support\n",
    "        raise NotImplementedError(\"Parquet requires pyarrow - use CSV for this demo\")\n",
    "    \n",
    "    def write_metadata(self, path: str, metadata: Dict[str, Any]) -> None:\n",
    "        \"\"\"Write metadata JSON file.\"\"\"\n",
    "        data = json.dumps(metadata, indent=2, default=str).encode('utf-8')\n",
    "        self.storage.write(path, data)\n",
    "    \n",
    "    def read_metadata(self, path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Read metadata JSON file.\"\"\"\n",
    "        data = self.storage.read(path)\n",
    "        return json.loads(data.decode('utf-8'))\n",
    "\n",
    "\n",
    "# Create a temporary data lake for this session\n",
    "TEMP_DIR = tempfile.mkdtemp(prefix='data_pipeline_')\n",
    "storage = LocalStorageBackend(TEMP_DIR)\n",
    "data_lake = DataLake(storage)\n",
    "\n",
    "print(f\"\\nData Lake initialized at: {TEMP_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Data Ingestion Layer\n",
    "\n",
    "The ingestion layer handles:\n",
    "- Generating/receiving raw data\n",
    "- Initial data validation\n",
    "- Storing in the landing zone (raw layer)\n",
    "\n",
    "We'll simulate multiple data sources with different characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataSource:\n",
    "    \"\"\"\n",
    "    Configuration for a data source.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    name : str\n",
    "        Unique identifier for the source\n",
    "    source_type : str\n",
    "        Type of source (api, database, file, stream)\n",
    "    schema : Dict[str, str]\n",
    "        Expected column names and data types\n",
    "    required_columns : List[str]\n",
    "        Columns that must be present\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    source_type: str\n",
    "    schema: Dict[str, str]\n",
    "    required_columns: List[str]\n",
    "    refresh_frequency: str = \"daily\"\n",
    "\n",
    "\n",
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    Simulates data from various sources.\n",
    "    \n",
    "    In production, this would be replaced with actual connectors:\n",
    "    - API clients\n",
    "    - Database connections\n",
    "    - Message queue consumers\n",
    "    - File watchers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seed: int = 42):\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def generate_transactions(self, n_records: int, date: datetime) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate simulated transaction data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_records : int\n",
    "            Number of records to generate\n",
    "        date : datetime\n",
    "            Date for the transactions\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Simulated transaction data\n",
    "        \"\"\"\n",
    "        products = ['Widget A', 'Widget B', 'Gadget X', 'Gadget Y', 'Service Z']\n",
    "        regions = ['US-East', 'US-West', 'EU', 'APAC']\n",
    "        \n",
    "        # Generate timestamps throughout the day\n",
    "        timestamps = [\n",
    "            date + timedelta(\n",
    "                hours=self.rng.integers(0, 24),\n",
    "                minutes=self.rng.integers(0, 60),\n",
    "                seconds=self.rng.integers(0, 60)\n",
    "            )\n",
    "            for _ in range(n_records)\n",
    "        ]\n",
    "        \n",
    "        data = {\n",
    "            'transaction_id': [f'TXN{date.strftime(\"%Y%m%d\")}{i:06d}' for i in range(n_records)],\n",
    "            'timestamp': timestamps,\n",
    "            'customer_id': [f'CUST{self.rng.integers(1000, 9999)}' for _ in range(n_records)],\n",
    "            'product': self.rng.choice(products, n_records),\n",
    "            'quantity': self.rng.integers(1, 10, n_records),\n",
    "            'unit_price': np.round(self.rng.uniform(10, 500, n_records), 2),\n",
    "            'region': self.rng.choice(regions, n_records),\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df['total_amount'] = df['quantity'] * df['unit_price']\n",
    "        \n",
    "        # Add some data quality issues for realistic testing\n",
    "        # 2% null values in region\n",
    "        null_idx = self.rng.choice(n_records, int(n_records * 0.02), replace=False)\n",
    "        df.loc[null_idx, 'region'] = None\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def generate_customer_data(self, n_customers: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate simulated customer master data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_customers : int\n",
    "            Number of customers\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Customer dimension data\n",
    "        \"\"\"\n",
    "        segments = ['Bronze', 'Silver', 'Gold', 'Platinum']\n",
    "        segment_weights = [0.4, 0.35, 0.2, 0.05]\n",
    "        \n",
    "        data = {\n",
    "            'customer_id': [f'CUST{i+1000}' for i in range(n_customers)],\n",
    "            'customer_name': [f'Customer {i+1}' for i in range(n_customers)],\n",
    "            'segment': self.rng.choice(segments, n_customers, p=segment_weights),\n",
    "            'join_date': [\n",
    "                datetime(2020, 1, 1) + timedelta(days=int(self.rng.integers(0, 1500)))\n",
    "                for _ in range(n_customers)\n",
    "            ],\n",
    "            'lifetime_value': np.round(self.rng.exponential(500, n_customers), 2)\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def generate_product_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate product catalog data.\"\"\"\n",
    "        return pd.DataFrame({\n",
    "            'product': ['Widget A', 'Widget B', 'Gadget X', 'Gadget Y', 'Service Z'],\n",
    "            'category': ['Hardware', 'Hardware', 'Electronics', 'Electronics', 'Services'],\n",
    "            'cost': [25.00, 45.00, 120.00, 85.00, 50.00],\n",
    "            'active': [True, True, True, True, True]\n",
    "        })\n",
    "\n",
    "\n",
    "# Define data sources\n",
    "SOURCES = {\n",
    "    'transactions': DataSource(\n",
    "        name='transactions',\n",
    "        source_type='stream',\n",
    "        schema={\n",
    "            'transaction_id': 'string',\n",
    "            'timestamp': 'datetime',\n",
    "            'customer_id': 'string',\n",
    "            'product': 'string',\n",
    "            'quantity': 'int',\n",
    "            'unit_price': 'float',\n",
    "            'region': 'string',\n",
    "            'total_amount': 'float'\n",
    "        },\n",
    "        required_columns=['transaction_id', 'timestamp', 'customer_id', 'product', 'quantity'],\n",
    "        refresh_frequency='hourly'\n",
    "    ),\n",
    "    'customers': DataSource(\n",
    "        name='customers',\n",
    "        source_type='database',\n",
    "        schema={\n",
    "            'customer_id': 'string',\n",
    "            'customer_name': 'string',\n",
    "            'segment': 'string',\n",
    "            'join_date': 'datetime',\n",
    "            'lifetime_value': 'float'\n",
    "        },\n",
    "        required_columns=['customer_id', 'segment'],\n",
    "        refresh_frequency='daily'\n",
    "    ),\n",
    "    'products': DataSource(\n",
    "        name='products',\n",
    "        source_type='file',\n",
    "        schema={\n",
    "            'product': 'string',\n",
    "            'category': 'string',\n",
    "            'cost': 'float',\n",
    "            'active': 'bool'\n",
    "        },\n",
    "        required_columns=['product', 'category', 'cost'],\n",
    "        refresh_frequency='weekly'\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Data Sources Configured:\")\n",
    "for name, source in SOURCES.items():\n",
    "    print(f\"  - {name}: {source.source_type} ({source.refresh_frequency})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ingester:\n",
    "    \"\"\"\n",
    "    Handles data ingestion with validation and landing zone storage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_lake : DataLake\n",
    "        Data lake interface for storage\n",
    "    landing_zone : str\n",
    "        Path prefix for raw data landing zone\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_lake: DataLake, landing_zone: str = \"landing\"):\n",
    "        self.data_lake = data_lake\n",
    "        self.landing_zone = landing_zone\n",
    "        self.generator = DataGenerator()\n",
    "    \n",
    "    def _validate_schema(self, df: pd.DataFrame, source: DataSource) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validate DataFrame against source schema.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Data to validate\n",
    "        source : DataSource\n",
    "            Source configuration with expected schema\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Validation results\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'valid': True,\n",
    "            'errors': [],\n",
    "            'warnings': [],\n",
    "            'row_count': len(df),\n",
    "            'column_count': len(df.columns)\n",
    "        }\n",
    "        \n",
    "        # Check required columns\n",
    "        missing_cols = set(source.required_columns) - set(df.columns)\n",
    "        if missing_cols:\n",
    "            results['valid'] = False\n",
    "            results['errors'].append(f\"Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        # Check for unexpected columns\n",
    "        extra_cols = set(df.columns) - set(source.schema.keys())\n",
    "        if extra_cols:\n",
    "            results['warnings'].append(f\"Unexpected columns: {extra_cols}\")\n",
    "        \n",
    "        # Check for null values in required columns\n",
    "        for col in source.required_columns:\n",
    "            if col in df.columns and df[col].isnull().any():\n",
    "                null_count = df[col].isnull().sum()\n",
    "                results['warnings'].append(f\"Column '{col}' has {null_count} null values\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def ingest(\n",
    "        self, \n",
    "        source_name: str, \n",
    "        date: datetime,\n",
    "        n_records: int = 1000\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Ingest data from a source into the landing zone.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        source_name : str\n",
    "            Name of the data source\n",
    "        date : datetime\n",
    "            Date for the data batch\n",
    "        n_records : int\n",
    "            Number of records (for generated data)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Ingestion results including path and validation\n",
    "        \"\"\"\n",
    "        source = SOURCES.get(source_name)\n",
    "        if not source:\n",
    "            raise ValueError(f\"Unknown source: {source_name}\")\n",
    "        \n",
    "        logger.info(f\"Starting ingestion for {source_name}\")\n",
    "        \n",
    "        # Generate/fetch data (in production, this would call actual sources)\n",
    "        if source_name == 'transactions':\n",
    "            df = self.generator.generate_transactions(n_records, date)\n",
    "        elif source_name == 'customers':\n",
    "            df = self.generator.generate_customer_data(n_records)\n",
    "        elif source_name == 'products':\n",
    "            df = self.generator.generate_product_data()\n",
    "        else:\n",
    "            raise ValueError(f\"No generator for source: {source_name}\")\n",
    "        \n",
    "        # Validate\n",
    "        validation = self._validate_schema(df, source)\n",
    "        \n",
    "        if not validation['valid']:\n",
    "            logger.error(f\"Validation failed: {validation['errors']}\")\n",
    "            return {\n",
    "                'success': False,\n",
    "                'source': source_name,\n",
    "                'validation': validation\n",
    "            }\n",
    "        \n",
    "        # Generate storage path (partitioned by date)\n",
    "        date_partition = date.strftime('%Y/%m/%d')\n",
    "        filename = f\"{source_name}_{date.strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        path = f\"{self.landing_zone}/{source_name}/{date_partition}/{filename}\"\n",
    "        \n",
    "        # Write to landing zone\n",
    "        self.data_lake.write_csv(df, path)\n",
    "        \n",
    "        # Write metadata\n",
    "        metadata = {\n",
    "            'source': source_name,\n",
    "            'ingestion_time': datetime.now().isoformat(),\n",
    "            'date_partition': date_partition,\n",
    "            'row_count': len(df),\n",
    "            'columns': list(df.columns),\n",
    "            'validation': validation,\n",
    "            'checksum': hashlib.md5(df.to_csv().encode()).hexdigest()\n",
    "        }\n",
    "        metadata_path = path.replace('.csv', '_metadata.json')\n",
    "        self.data_lake.write_metadata(metadata_path, metadata)\n",
    "        \n",
    "        logger.info(f\"Ingested {len(df)} rows to {path}\")\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'source': source_name,\n",
    "            'path': path,\n",
    "            'row_count': len(df),\n",
    "            'validation': validation\n",
    "        }\n",
    "\n",
    "\n",
    "# Create ingester and ingest some data\n",
    "ingester = Ingester(data_lake)\n",
    "\n",
    "# Ingest data for multiple dates\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA INGESTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ingestion_results = []\n",
    "base_date = datetime(2024, 1, 15)\n",
    "\n",
    "# Ingest transactions for 3 days\n",
    "for day_offset in range(3):\n",
    "    date = base_date + timedelta(days=day_offset)\n",
    "    result = ingester.ingest('transactions', date, n_records=500)\n",
    "    ingestion_results.append(result)\n",
    "    print(f\"  Transactions {date.date()}: {result['row_count']} rows\")\n",
    "\n",
    "# Ingest dimension tables\n",
    "result = ingester.ingest('customers', base_date, n_records=200)\n",
    "ingestion_results.append(result)\n",
    "print(f\"  Customers: {result['row_count']} rows\")\n",
    "\n",
    "result = ingester.ingest('products', base_date)\n",
    "ingestion_results.append(result)\n",
    "print(f\"  Products: {result['row_count']} rows\")\n",
    "\n",
    "# Show landing zone contents\n",
    "print(\"\\nLanding Zone Contents:\")\n",
    "for obj in storage.list_objects('landing'):\n",
    "    if not obj.endswith('_metadata.json'):\n",
    "        print(f\"  {obj}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Data Transformation Layer\n",
    "\n",
    "The transformation layer implements business logic:\n",
    "- Data cleaning and standardization\n",
    "- Business rule application\n",
    "- Aggregations and calculations\n",
    "- Data enrichment through joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformationResult:\n",
    "    \"\"\"\n",
    "    Result of a transformation step.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    success : bool\n",
    "        Whether the transformation succeeded\n",
    "    input_rows : int\n",
    "        Number of input rows\n",
    "    output_rows : int\n",
    "        Number of output rows\n",
    "    dropped_rows : int\n",
    "        Number of rows dropped during transformation\n",
    "    output_path : str\n",
    "        Where the output was stored\n",
    "    metrics : Dict[str, Any]\n",
    "        Additional metrics from the transformation\n",
    "    \"\"\"\n",
    "    success: bool\n",
    "    input_rows: int\n",
    "    output_rows: int\n",
    "    dropped_rows: int = 0\n",
    "    output_path: str = \"\"\n",
    "    metrics: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "    \"\"\"\n",
    "    Handles data transformations from landing zone to processed zone.\n",
    "    \n",
    "    Implements the transformation layer of the data pipeline.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_lake : DataLake\n",
    "        Data lake interface\n",
    "    landing_zone : str\n",
    "        Path to landing zone\n",
    "    processed_zone : str\n",
    "        Path to processed zone\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        data_lake: DataLake, \n",
    "        landing_zone: str = \"landing\",\n",
    "        processed_zone: str = \"processed\"\n",
    "    ):\n",
    "        self.data_lake = data_lake\n",
    "        self.landing_zone = landing_zone\n",
    "        self.processed_zone = processed_zone\n",
    "    \n",
    "    def _load_latest_landing_data(self, source_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load all data from landing zone for a source.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        source_name : str\n",
    "            Name of the data source\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Combined data from all partitions\n",
    "        \"\"\"\n",
    "        prefix = f\"{self.landing_zone}/{source_name}\"\n",
    "        files = [\n",
    "            f for f in self.data_lake.storage.list_objects(prefix)\n",
    "            if f.endswith('.csv') and '_metadata' not in f\n",
    "        ]\n",
    "        \n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No data files found for {source_name}\")\n",
    "        \n",
    "        dfs = [self.data_lake.read_csv(f) for f in files]\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    def clean_transactions(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Clean and standardize transaction data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Raw transaction data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (cleaned DataFrame, cleaning metrics)\n",
    "        \"\"\"\n",
    "        metrics = {'initial_rows': len(df)}\n",
    "        \n",
    "        # Remove duplicates\n",
    "        df = df.drop_duplicates(subset=['transaction_id'])\n",
    "        metrics['after_dedup'] = len(df)\n",
    "        \n",
    "        # Handle missing regions (fill with 'Unknown')\n",
    "        null_regions = df['region'].isnull().sum()\n",
    "        df['region'] = df['region'].fillna('Unknown')\n",
    "        metrics['null_regions_filled'] = null_regions\n",
    "        \n",
    "        # Standardize product names\n",
    "        df['product'] = df['product'].str.strip().str.title()\n",
    "        \n",
    "        # Parse timestamp\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        # Add derived columns\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        df['hour'] = df['timestamp'].dt.hour\n",
    "        df['day_of_week'] = df['timestamp'].dt.day_name()\n",
    "        df['is_weekend'] = df['timestamp'].dt.dayofweek >= 5\n",
    "        \n",
    "        # Remove invalid transactions (negative amounts)\n",
    "        invalid_mask = df['total_amount'] < 0\n",
    "        metrics['invalid_amounts_removed'] = invalid_mask.sum()\n",
    "        df = df[~invalid_mask]\n",
    "        \n",
    "        metrics['final_rows'] = len(df)\n",
    "        \n",
    "        return df, metrics\n",
    "    \n",
    "    def enrich_transactions(\n",
    "        self, \n",
    "        transactions: pd.DataFrame,\n",
    "        customers: pd.DataFrame,\n",
    "        products: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Enrich transaction data with customer and product information.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        transactions : pd.DataFrame\n",
    "            Cleaned transaction data\n",
    "        customers : pd.DataFrame\n",
    "            Customer dimension data\n",
    "        products : pd.DataFrame\n",
    "            Product dimension data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Enriched transaction data\n",
    "        \"\"\"\n",
    "        # Join with customers\n",
    "        df = transactions.merge(\n",
    "            customers[['customer_id', 'segment', 'lifetime_value']],\n",
    "            on='customer_id',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Fill missing customer info\n",
    "        df['segment'] = df['segment'].fillna('Unknown')\n",
    "        df['lifetime_value'] = df['lifetime_value'].fillna(0)\n",
    "        \n",
    "        # Join with products\n",
    "        df = df.merge(\n",
    "            products[['product', 'category', 'cost']],\n",
    "            on='product',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Calculate profit metrics\n",
    "        df['profit'] = df['total_amount'] - (df['cost'].fillna(0) * df['quantity'])\n",
    "        df['profit_margin'] = (df['profit'] / df['total_amount'] * 100).round(2)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_daily_summary(\n",
    "        self, \n",
    "        enriched_transactions: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create daily summary aggregations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        enriched_transactions : pd.DataFrame\n",
    "            Enriched transaction data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Daily summary metrics\n",
    "        \"\"\"\n",
    "        summary = enriched_transactions.groupby(['date', 'region', 'category']).agg({\n",
    "            'transaction_id': 'count',\n",
    "            'customer_id': 'nunique',\n",
    "            'quantity': 'sum',\n",
    "            'total_amount': 'sum',\n",
    "            'profit': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        summary.columns = [\n",
    "            'date', 'region', 'category', \n",
    "            'transaction_count', 'unique_customers',\n",
    "            'total_quantity', 'revenue', 'profit'\n",
    "        ]\n",
    "        \n",
    "        summary['avg_transaction_value'] = (summary['revenue'] / summary['transaction_count']).round(2)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def run_transformation_pipeline(self) -> Dict[str, TransformationResult]:\n",
    "        \"\"\"\n",
    "        Run the complete transformation pipeline.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Results for each transformation step\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        logger.info(\"Starting transformation pipeline\")\n",
    "        \n",
    "        # Step 1: Load raw data\n",
    "        logger.info(\"Loading raw data from landing zone\")\n",
    "        raw_transactions = self._load_latest_landing_data('transactions')\n",
    "        customers = self._load_latest_landing_data('customers')\n",
    "        products = self._load_latest_landing_data('products')\n",
    "        \n",
    "        # Step 2: Clean transactions\n",
    "        logger.info(\"Cleaning transaction data\")\n",
    "        cleaned_transactions, clean_metrics = self.clean_transactions(raw_transactions)\n",
    "        \n",
    "        clean_path = f\"{self.processed_zone}/cleaned/transactions.csv\"\n",
    "        self.data_lake.write_csv(cleaned_transactions, clean_path)\n",
    "        \n",
    "        results['clean'] = TransformationResult(\n",
    "            success=True,\n",
    "            input_rows=clean_metrics['initial_rows'],\n",
    "            output_rows=clean_metrics['final_rows'],\n",
    "            dropped_rows=clean_metrics['initial_rows'] - clean_metrics['final_rows'],\n",
    "            output_path=clean_path,\n",
    "            metrics=clean_metrics\n",
    "        )\n",
    "        \n",
    "        # Step 3: Enrich transactions\n",
    "        logger.info(\"Enriching transaction data\")\n",
    "        enriched = self.enrich_transactions(cleaned_transactions, customers, products)\n",
    "        \n",
    "        enriched_path = f\"{self.processed_zone}/enriched/transactions.csv\"\n",
    "        self.data_lake.write_csv(enriched, enriched_path)\n",
    "        \n",
    "        results['enrich'] = TransformationResult(\n",
    "            success=True,\n",
    "            input_rows=len(cleaned_transactions),\n",
    "            output_rows=len(enriched),\n",
    "            output_path=enriched_path,\n",
    "            metrics={\n",
    "                'customers_matched': (enriched['segment'] != 'Unknown').sum(),\n",
    "                'products_matched': enriched['category'].notna().sum()\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Step 4: Create aggregations\n",
    "        logger.info(\"Creating daily summaries\")\n",
    "        daily_summary = self.create_daily_summary(enriched)\n",
    "        \n",
    "        summary_path = f\"{self.processed_zone}/aggregated/daily_summary.csv\"\n",
    "        self.data_lake.write_csv(daily_summary, summary_path)\n",
    "        \n",
    "        results['aggregate'] = TransformationResult(\n",
    "            success=True,\n",
    "            input_rows=len(enriched),\n",
    "            output_rows=len(daily_summary),\n",
    "            output_path=summary_path,\n",
    "            metrics={\n",
    "                'total_revenue': enriched['total_amount'].sum(),\n",
    "                'total_profit': enriched['profit'].sum(),\n",
    "                'unique_dates': enriched['date'].nunique()\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Transformation pipeline complete\")\n",
    "        return results\n",
    "\n",
    "\n",
    "# Run transformations\n",
    "transformer = Transformer(data_lake)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA TRANSFORMATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "transform_results = transformer.run_transformation_pipeline()\n",
    "\n",
    "print(\"\\nTransformation Results:\")\n",
    "for step, result in transform_results.items():\n",
    "    print(f\"\\n  {step.upper()}:\")\n",
    "    print(f\"    Input rows: {result.input_rows}\")\n",
    "    print(f\"    Output rows: {result.output_rows}\")\n",
    "    if result.dropped_rows > 0:\n",
    "        print(f\"    Dropped rows: {result.dropped_rows}\")\n",
    "    print(f\"    Output: {result.output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View transformed data samples\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRANSFORMED DATA PREVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load and display enriched transactions\n",
    "enriched_df = data_lake.read_csv('processed/enriched/transactions.csv')\n",
    "print(f\"\\nEnriched Transactions ({len(enriched_df)} rows):\")\n",
    "print(enriched_df.head(10).to_string())\n",
    "\n",
    "# Load and display daily summary\n",
    "summary_df = data_lake.read_csv('processed/aggregated/daily_summary.csv')\n",
    "print(f\"\\n\\nDaily Summary ({len(summary_df)} rows):\")\n",
    "print(summary_df.head(20).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Data Quality & Validation Layer\n",
    "\n",
    "A critical aspect of production pipelines is continuous data quality monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityCheckType(Enum):\n",
    "    \"\"\"Types of data quality checks.\"\"\"\n",
    "    COMPLETENESS = \"completeness\"\n",
    "    UNIQUENESS = \"uniqueness\"\n",
    "    VALIDITY = \"validity\"\n",
    "    CONSISTENCY = \"consistency\"\n",
    "    TIMELINESS = \"timeliness\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QualityCheck:\n",
    "    \"\"\"\n",
    "    Definition of a data quality check.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    name : str\n",
    "        Descriptive name of the check\n",
    "    check_type : QualityCheckType\n",
    "        Category of quality check\n",
    "    column : str\n",
    "        Column to check (if applicable)\n",
    "    check_fn : Callable\n",
    "        Function that performs the check\n",
    "    threshold : float\n",
    "        Minimum acceptable score (0-1)\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    check_type: QualityCheckType\n",
    "    column: Optional[str]\n",
    "    check_fn: Callable[[pd.DataFrame], float]\n",
    "    threshold: float = 0.95\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QualityReport:\n",
    "    \"\"\"\n",
    "    Results of quality checks on a dataset.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    dataset_name : str\n",
    "        Name of the checked dataset\n",
    "    timestamp : datetime\n",
    "        When the checks were run\n",
    "    overall_score : float\n",
    "        Average score across all checks\n",
    "    passed : bool\n",
    "        Whether all checks passed their thresholds\n",
    "    check_results : List[Dict]\n",
    "        Detailed results for each check\n",
    "    \"\"\"\n",
    "    dataset_name: str\n",
    "    timestamp: datetime\n",
    "    overall_score: float\n",
    "    passed: bool\n",
    "    check_results: List[Dict[str, Any]]\n",
    "\n",
    "\n",
    "class DataQualityChecker:\n",
    "    \"\"\"\n",
    "    Performs data quality checks and generates reports.\n",
    "    \n",
    "    Provides a framework for defining and running quality checks\n",
    "    on DataFrames, producing detailed reports.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.checks: List[QualityCheck] = []\n",
    "    \n",
    "    def add_check(self, check: QualityCheck) -> None:\n",
    "        \"\"\"Add a quality check to run.\"\"\"\n",
    "        self.checks.append(check)\n",
    "    \n",
    "    def add_completeness_check(\n",
    "        self, \n",
    "        column: str, \n",
    "        threshold: float = 0.95\n",
    "    ) -> None:\n",
    "        \"\"\"Add a check for column completeness (non-null rate).\"\"\"\n",
    "        self.add_check(QualityCheck(\n",
    "            name=f\"Completeness: {column}\",\n",
    "            check_type=QualityCheckType.COMPLETENESS,\n",
    "            column=column,\n",
    "            check_fn=lambda df, col=column: 1 - df[col].isnull().mean(),\n",
    "            threshold=threshold\n",
    "        ))\n",
    "    \n",
    "    def add_uniqueness_check(\n",
    "        self, \n",
    "        column: str, \n",
    "        threshold: float = 1.0\n",
    "    ) -> None:\n",
    "        \"\"\"Add a check for column uniqueness.\"\"\"\n",
    "        self.add_check(QualityCheck(\n",
    "            name=f\"Uniqueness: {column}\",\n",
    "            check_type=QualityCheckType.UNIQUENESS,\n",
    "            column=column,\n",
    "            check_fn=lambda df, col=column: df[col].nunique() / len(df),\n",
    "            threshold=threshold\n",
    "        ))\n",
    "    \n",
    "    def add_range_check(\n",
    "        self, \n",
    "        column: str, \n",
    "        min_val: float, \n",
    "        max_val: float,\n",
    "        threshold: float = 0.99\n",
    "    ) -> None:\n",
    "        \"\"\"Add a check for values within expected range.\"\"\"\n",
    "        self.add_check(QualityCheck(\n",
    "            name=f\"Range: {column} [{min_val}, {max_val}]\",\n",
    "            check_type=QualityCheckType.VALIDITY,\n",
    "            column=column,\n",
    "            check_fn=lambda df, col=column, mn=min_val, mx=max_val: \n",
    "                ((df[col] >= mn) & (df[col] <= mx)).mean(),\n",
    "            threshold=threshold\n",
    "        ))\n",
    "    \n",
    "    def add_custom_check(\n",
    "        self,\n",
    "        name: str,\n",
    "        check_fn: Callable[[pd.DataFrame], float],\n",
    "        check_type: QualityCheckType = QualityCheckType.VALIDITY,\n",
    "        threshold: float = 0.95\n",
    "    ) -> None:\n",
    "        \"\"\"Add a custom quality check.\"\"\"\n",
    "        self.add_check(QualityCheck(\n",
    "            name=name,\n",
    "            check_type=check_type,\n",
    "            column=None,\n",
    "            check_fn=check_fn,\n",
    "            threshold=threshold\n",
    "        ))\n",
    "    \n",
    "    def run_checks(self, df: pd.DataFrame, dataset_name: str) -> QualityReport:\n",
    "        \"\"\"\n",
    "        Run all configured quality checks on a DataFrame.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Data to check\n",
    "        dataset_name : str\n",
    "            Name for the report\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        QualityReport\n",
    "            Detailed quality report\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        all_passed = True\n",
    "        \n",
    "        for check in self.checks:\n",
    "            try:\n",
    "                score = check.check_fn(df)\n",
    "                passed = score >= check.threshold\n",
    "                status = \"PASS\" if passed else \"FAIL\"\n",
    "                \n",
    "                if not passed:\n",
    "                    all_passed = False\n",
    "                \n",
    "                results.append({\n",
    "                    'check_name': check.name,\n",
    "                    'check_type': check.check_type.value,\n",
    "                    'score': round(score, 4),\n",
    "                    'threshold': check.threshold,\n",
    "                    'status': status\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    'check_name': check.name,\n",
    "                    'check_type': check.check_type.value,\n",
    "                    'score': 0,\n",
    "                    'threshold': check.threshold,\n",
    "                    'status': f\"ERROR: {str(e)}\"\n",
    "                })\n",
    "                all_passed = False\n",
    "        \n",
    "        overall_score = np.mean([r['score'] for r in results if isinstance(r['score'], (int, float))])\n",
    "        \n",
    "        return QualityReport(\n",
    "            dataset_name=dataset_name,\n",
    "            timestamp=datetime.now(),\n",
    "            overall_score=round(overall_score, 4),\n",
    "            passed=all_passed,\n",
    "            check_results=results\n",
    "        )\n",
    "\n",
    "\n",
    "# Create and configure quality checker\n",
    "checker = DataQualityChecker()\n",
    "\n",
    "# Add checks for transaction data\n",
    "checker.add_uniqueness_check('transaction_id', threshold=1.0)\n",
    "checker.add_completeness_check('customer_id', threshold=1.0)\n",
    "checker.add_completeness_check('product', threshold=1.0)\n",
    "checker.add_completeness_check('region', threshold=0.95)  # Allow some missing\n",
    "checker.add_range_check('quantity', 1, 100, threshold=0.99)\n",
    "checker.add_range_check('total_amount', 0, 10000, threshold=0.99)\n",
    "checker.add_range_check('profit_margin', -50, 100, threshold=0.95)\n",
    "\n",
    "# Add custom business rule checks\n",
    "checker.add_custom_check(\n",
    "    name=\"Positive revenue\",\n",
    "    check_fn=lambda df: (df['total_amount'] > 0).mean(),\n",
    "    threshold=0.99\n",
    ")\n",
    "\n",
    "checker.add_custom_check(\n",
    "    name=\"Valid segments\",\n",
    "    check_fn=lambda df: df['segment'].isin(['Bronze', 'Silver', 'Gold', 'Platinum', 'Unknown']).mean(),\n",
    "    threshold=1.0\n",
    ")\n",
    "\n",
    "# Run quality checks\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "report = checker.run_checks(enriched_df, \"Enriched Transactions\")\n",
    "\n",
    "print(f\"\\nDataset: {report.dataset_name}\")\n",
    "print(f\"Timestamp: {report.timestamp}\")\n",
    "print(f\"Overall Score: {report.overall_score:.2%}\")\n",
    "print(f\"Status: {'PASSED' if report.passed else 'FAILED'}\")\n",
    "\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Check Name':<35} {'Type':<15} {'Score':<10} {'Status':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for result in report.check_results:\n",
    "    status_icon = \"[OK]\" if result['status'] == 'PASS' else \"[X]\"\n",
    "    print(f\"{result['check_name']:<35} {result['check_type']:<15} {result['score']:.2%}    {status_icon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Pipeline Orchestration\n",
    "\n",
    "Finally, we'll create an orchestration layer that ties everything together and provides:\n",
    "- End-to-end pipeline execution\n",
    "- Error handling and recovery\n",
    "- Execution logging and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineStatus(Enum):\n",
    "    \"\"\"Status of a pipeline run.\"\"\"\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\"\n",
    "    SUCCESS = \"success\"\n",
    "    FAILED = \"failed\"\n",
    "    PARTIAL = \"partial\"  # Some steps failed\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineStep:\n",
    "    \"\"\"\n",
    "    Definition of a pipeline step.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    name : str\n",
    "        Step identifier\n",
    "    description : str\n",
    "        What this step does\n",
    "    execute_fn : Callable\n",
    "        Function to execute\n",
    "    required : bool\n",
    "        Whether failure should stop the pipeline\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    execute_fn: Callable[[], Any]\n",
    "    required: bool = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineRun:\n",
    "    \"\"\"\n",
    "    Record of a pipeline execution.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    run_id : str\n",
    "        Unique identifier for this run\n",
    "    pipeline_name : str\n",
    "        Name of the pipeline\n",
    "    status : PipelineStatus\n",
    "        Current status\n",
    "    start_time : datetime\n",
    "        When the run started\n",
    "    end_time : Optional[datetime]\n",
    "        When the run ended (if complete)\n",
    "    step_results : Dict[str, Any]\n",
    "        Results from each step\n",
    "    \"\"\"\n",
    "    run_id: str\n",
    "    pipeline_name: str\n",
    "    status: PipelineStatus\n",
    "    start_time: datetime\n",
    "    end_time: Optional[datetime] = None\n",
    "    step_results: Dict[str, Any] = field(default_factory=dict)\n",
    "    errors: List[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "class DataPipeline:\n",
    "    \"\"\"\n",
    "    Orchestrates the complete data pipeline.\n",
    "    \n",
    "    Manages the execution of pipeline steps with proper\n",
    "    error handling, logging, and state tracking.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        Pipeline name\n",
    "    data_lake : DataLake\n",
    "        Data lake interface\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, data_lake: DataLake):\n",
    "        self.name = name\n",
    "        self.data_lake = data_lake\n",
    "        self.steps: List[PipelineStep] = []\n",
    "        self.runs: List[PipelineRun] = []\n",
    "    \n",
    "    def add_step(self, step: PipelineStep) -> None:\n",
    "        \"\"\"Add a step to the pipeline.\"\"\"\n",
    "        self.steps.append(step)\n",
    "        logger.debug(f\"Added step: {step.name}\")\n",
    "    \n",
    "    def _generate_run_id(self) -> str:\n",
    "        \"\"\"Generate a unique run ID.\"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        return f\"{self.name}_{timestamp}\"\n",
    "    \n",
    "    def run(self) -> PipelineRun:\n",
    "        \"\"\"\n",
    "        Execute the pipeline.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        PipelineRun\n",
    "            Record of the pipeline execution\n",
    "        \"\"\"\n",
    "        run = PipelineRun(\n",
    "            run_id=self._generate_run_id(),\n",
    "            pipeline_name=self.name,\n",
    "            status=PipelineStatus.RUNNING,\n",
    "            start_time=datetime.now()\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Starting pipeline run: {run.run_id}\")\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PIPELINE RUN: {run.run_id}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        all_success = True\n",
    "        \n",
    "        for i, step in enumerate(self.steps, 1):\n",
    "            print(f\"Step {i}/{len(self.steps)}: {step.name}\")\n",
    "            print(f\"  Description: {step.description}\")\n",
    "            \n",
    "            try:\n",
    "                step_start = datetime.now()\n",
    "                result = step.execute_fn()\n",
    "                step_duration = (datetime.now() - step_start).total_seconds()\n",
    "                \n",
    "                run.step_results[step.name] = {\n",
    "                    'status': 'success',\n",
    "                    'duration_seconds': step_duration,\n",
    "                    'result': result\n",
    "                }\n",
    "                print(f\"  Status: SUCCESS ({step_duration:.2f}s)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"{step.name}: {str(e)}\"\n",
    "                run.errors.append(error_msg)\n",
    "                run.step_results[step.name] = {\n",
    "                    'status': 'failed',\n",
    "                    'error': str(e)\n",
    "                }\n",
    "                print(f\"  Status: FAILED - {str(e)}\")\n",
    "                \n",
    "                if step.required:\n",
    "                    all_success = False\n",
    "                    logger.error(f\"Required step failed: {step.name}\")\n",
    "                    # Don't stop, continue to try other steps for partial completion\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        # Determine final status\n",
    "        run.end_time = datetime.now()\n",
    "        \n",
    "        if len(run.errors) == 0:\n",
    "            run.status = PipelineStatus.SUCCESS\n",
    "        elif all_success:\n",
    "            run.status = PipelineStatus.PARTIAL  # Non-required steps failed\n",
    "        else:\n",
    "            run.status = PipelineStatus.FAILED\n",
    "        \n",
    "        # Store run record\n",
    "        self.runs.append(run)\n",
    "        \n",
    "        # Save run metadata\n",
    "        metadata_path = f\"pipeline_runs/{run.run_id}_metadata.json\"\n",
    "        self.data_lake.write_metadata(metadata_path, asdict(run))\n",
    "        \n",
    "        duration = (run.end_time - run.start_time).total_seconds()\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"PIPELINE COMPLETE\")\n",
    "        print(f\"  Status: {run.status.value.upper()}\")\n",
    "        print(f\"  Duration: {duration:.2f} seconds\")\n",
    "        print(f\"  Steps: {len(self.steps)} total, {len(run.errors)} failed\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return run\n",
    "\n",
    "\n",
    "# Create the main pipeline\n",
    "pipeline = DataPipeline(\"sales_etl\", data_lake)\n",
    "\n",
    "# Create fresh ingester and transformer for pipeline\n",
    "pipeline_ingester = Ingester(data_lake)\n",
    "pipeline_transformer = Transformer(data_lake)\n",
    "pipeline_checker = DataQualityChecker()\n",
    "\n",
    "# Configure quality checker for pipeline\n",
    "pipeline_checker.add_uniqueness_check('transaction_id')\n",
    "pipeline_checker.add_completeness_check('customer_id')\n",
    "pipeline_checker.add_range_check('total_amount', 0, 10000)\n",
    "\n",
    "# Define pipeline steps\n",
    "processing_date = datetime(2024, 1, 20)\n",
    "\n",
    "pipeline.add_step(PipelineStep(\n",
    "    name=\"ingest_transactions\",\n",
    "    description=\"Ingest daily transaction data\",\n",
    "    execute_fn=lambda: pipeline_ingester.ingest('transactions', processing_date, n_records=300),\n",
    "    required=True\n",
    "))\n",
    "\n",
    "pipeline.add_step(PipelineStep(\n",
    "    name=\"ingest_customers\",\n",
    "    description=\"Ingest customer dimension data\",\n",
    "    execute_fn=lambda: pipeline_ingester.ingest('customers', processing_date, n_records=100),\n",
    "    required=True\n",
    "))\n",
    "\n",
    "pipeline.add_step(PipelineStep(\n",
    "    name=\"ingest_products\",\n",
    "    description=\"Ingest product catalog\",\n",
    "    execute_fn=lambda: pipeline_ingester.ingest('products', processing_date),\n",
    "    required=True\n",
    "))\n",
    "\n",
    "pipeline.add_step(PipelineStep(\n",
    "    name=\"transform_data\",\n",
    "    description=\"Run all data transformations\",\n",
    "    execute_fn=lambda: pipeline_transformer.run_transformation_pipeline(),\n",
    "    required=True\n",
    "))\n",
    "\n",
    "pipeline.add_step(PipelineStep(\n",
    "    name=\"quality_checks\",\n",
    "    description=\"Run data quality validation\",\n",
    "    execute_fn=lambda: pipeline_checker.run_checks(\n",
    "        data_lake.read_csv('processed/enriched/transactions.csv'),\n",
    "        \"Final Output\"\n",
    "    ),\n",
    "    required=False  # Quality check failure shouldn't stop pipeline\n",
    "))\n",
    "\n",
    "# Run the pipeline\n",
    "run_result = pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and data lake contents\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL DATA LAKE CONTENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_objects = storage.list_objects()\n",
    "\n",
    "# Group by zone\n",
    "zones = {}\n",
    "for obj in all_objects:\n",
    "    zone = obj.split('/')[0] if '/' in obj else 'root'\n",
    "    if zone not in zones:\n",
    "        zones[zone] = []\n",
    "    zones[zone].append(obj)\n",
    "\n",
    "for zone, files in sorted(zones.items()):\n",
    "    print(f\"\\n{zone.upper()}/\")\n",
    "    for f in files:\n",
    "        # Get file size\n",
    "        full_path = storage._full_path(f)\n",
    "        size = full_path.stat().st_size if full_path.exists() else 0\n",
    "        size_str = f\"{size:,} bytes\" if size < 1024 else f\"{size/1024:.1f} KB\"\n",
    "        print(f\"  {f.replace(zone + '/', '')} ({size_str})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary directory\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLEANUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Note: In a real scenario, you'd keep this data\n",
    "# For demo purposes, we can clean up\n",
    "try:\n",
    "    shutil.rmtree(TEMP_DIR)\n",
    "    print(f\"Cleaned up temporary directory: {TEMP_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"Cleanup note: {e}\")\n",
    "\n",
    "print(\"\\nPipeline demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Production Considerations\n",
    "\n",
    "This capstone project demonstrated key patterns for building data pipelines:\n",
    "\n",
    "### Patterns Implemented\n",
    "\n",
    "1. **Storage Abstraction**\n",
    "   - Protocol-based interface for storage backends\n",
    "   - Easy switching between local and cloud storage\n",
    "   - Consistent API regardless of backend\n",
    "\n",
    "2. **Data Lake Zones**\n",
    "   - Landing zone: Raw, immutable data\n",
    "   - Processed zone: Cleaned and transformed data\n",
    "   - Aggregated zone: Business-ready summaries\n",
    "\n",
    "3. **Data Quality**\n",
    "   - Configurable quality checks\n",
    "   - Automated validation reports\n",
    "   - Threshold-based pass/fail criteria\n",
    "\n",
    "4. **Pipeline Orchestration**\n",
    "   - Step-based execution\n",
    "   - Error handling and recovery\n",
    "   - Execution logging and metrics\n",
    "\n",
    "### Production Enhancements\n",
    "\n",
    "For production deployment, you would add:\n",
    "\n",
    "```python\n",
    "# 1. Use actual cloud storage\n",
    "import boto3\n",
    "class S3StorageBackend(StorageBackend):\n",
    "    def __init__(self, bucket: str):\n",
    "        self.s3 = boto3.client('s3')\n",
    "        self.bucket = bucket\n",
    "    # ... implement methods\n",
    "\n",
    "# 2. Use orchestration tools\n",
    "# - Apache Airflow for scheduling\n",
    "# - AWS Step Functions for serverless\n",
    "# - Prefect or Dagster for modern pipelines\n",
    "\n",
    "# 3. Add monitoring\n",
    "# - CloudWatch/DataDog metrics\n",
    "# - Alerting on failures\n",
    "# - Dashboard for pipeline health\n",
    "\n",
    "# 4. Implement partitioning\n",
    "# - Time-based partitions (year/month/day)\n",
    "# - Efficient incremental processing\n",
    "\n",
    "# 5. Add data catalog\n",
    "# - AWS Glue Catalog\n",
    "# - Schema registry\n",
    "# - Data lineage tracking\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Abstraction enables flexibility**: The storage abstraction allows the same code to work locally and in the cloud\n",
    "2. **Data quality is critical**: Automated checks catch issues before they propagate\n",
    "3. **Idempotency matters**: Pipelines should be re-runnable without side effects\n",
    "4. **Logging and metrics**: Essential for debugging and monitoring\n",
    "5. **Error handling**: Graceful degradation vs. fail-fast depends on use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"     CLOUD DATA PIPELINE CAPSTONE COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nYou have learned to:\")\n",
    "print(\"  [x] Create storage abstraction layers\")\n",
    "print(\"  [x] Design data ingestion patterns\")\n",
    "print(\"  [x] Implement ETL transformations\")\n",
    "print(\"  [x] Build data quality frameworks\")\n",
    "print(\"  [x] Orchestrate end-to-end pipelines\")\n",
    "print(\"  [x] Structure code for production deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
